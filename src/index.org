#+TITLE:     Building an application for the Analytics of Virtual Labs
#+AUTHOR:    VLEAD
#+DATE:      2015-09-09 Wed
#+PROPERTY: results output
#+PROPERTY: exports code
#+SETUPFILE: org-templates/level-0.org

* Introduction
  This model describes a simple web application which displays the analytics of
  virtual-labs.
* Requirements
** Requirement #0
   The "visitor page views" count of the labs deployed on Amazon Web Services(AWS) and IIIT infrastructure from both AWS and
   local deployment must be displayed on vlabs web page. The "visitor page views" count must be updated every one hour.
   Note: vlabs web page at time of this writing is [[http://www.vlabs.ac.in]]
** Requirement #1
   Each hit to a  simulation is a Usage Instance.
** Requirement #2
   Total Usage Hits = {hc, hs}
   where,
   hc = hit to an experiment content page.
   hs = hit to the same experiment simulation page.
   hc.ip = hs.ip
** Requirement #3
   Lab wise Usage Hits = {hc, hs}
   where,
   hc = hit to an experiment content page.
   hs = hit to the same experiment simulation page.
   hc.ip = hs.ip
   
* Design
** Design description
   This model builds a web service that provides endpoints to get
   different sets of information from the statistics generated by
   awstats and apache logs. A four tier architecture is built with the
   following components :
   - data : the specific data based on the requirement.
   - service : data processing and service to provide data
   - view : presentation of data.
   The tiers in the architecture are :
   + Collation of data
   + Parsing of data 
   + Service to provide data
   + Presentation of data
   
   The design of the architecture is depicted in the diagram below.
   
   #+CAPTION:  Design diagram
   #+LABEL:  Design diagram
   [[./diagrams/architecture.png]]

   The analytics service as shown in the above diagram has inputs
   coming from two sources. The first source is from the log files
   generated by the apache web server. The other input to the service is the awstats text
   files.
** Collation of data
    Collation of data can be found in the [[Analytics Server]].
** Parsing of data
     Awstats is a log analyzer which parses the logs and generates the
     analytics.  Since the logs are stored in the reverse proxy,
     awstats is installed on the same machine.  Awstats generates text
     files which contain the statistics in text format.  For each lab,
     a new file gets generated each month.  The daily statistics are
     updated in the monthly text file.  The required data from these
     text files needs to be extracted. Also the access log of the web
     server (apache) are required to extract certain data.
** Service to provide data
    A service exposes different endpoints. Currently the endpoint that
    is exposed provides total number of pages, usage. 
** Presentation
    To display the analytics on vlabs web page, Ajax call to the service is made from vlabs.ac.in web page
    and required service will return the calculated value in the form of json.
   
* Implementation
** Parsing of data
*** Page-Views extraction for Requirement0
    For [[Requirement%20#1][requirement #0]] the script does the following:
      + parse the awstats data files
      + extract the =number of pages viewed= from each data file
      + Add the total page views from all the files in AWS
      + Add the total page views from the specific files in VLEAD
      + Add the total number of pages viewed from AWS and VLEAD.
    The following script will be used to satisfy the [[Requirement #0]]
#+BEGIN_SRC python :tangle /analytics/extract_data.py
import config
import sys
import re
import json
import os
import fnmatch
__dict_tot_pages__ = {}
all = [__dict_tot_pages__ ]
#+END_SRC
#+BEGIN_SRC python :tangle /analytics/extract_data.py
def get_file_names(year,location):
    list_of_files = []
    for file in os.listdir(location):
        if fnmatch.fnmatch(file, '*'+ re.escape(year) + '*.txt'):
            list_of_files.append(location + file)
    return list_of_files
#+END_SRC
    The function below looks through only the specified files to obtain the page views.
    It returns the total page views of labs deployed on AWS and IIIT infrastructure.
#+BEGIN_SRC python :tangle /analytics/extract_data.py
def extract_data(year,location,data):
    files = get_file_names(year,location);
    dictionary = {}
    dict_tot_pages = {}
    for i in files:
        if data!= "NONE":
            if re.search(data,i):
                f = open(i)    
        else:
            f = open(i)
            for line in f:
                if re.match(r'^BEGIN_DAY',line):
                    n = int((line.strip('BEGIN_DAY ')).rstrip())
                    x = 0
                    p = 0
                    for page_line in f:
                        if (x >= n):
                            break
                        p += int(page_line.split()[1])
                        x += 1
                    f.close()
                    dictionary[(i.lstrip('awstats.')).rstrip('.txt')] = p
                    break
    z = 0
    for key in dictionary:
        z += dictionary[key]
    return z

#+END_SRC
    
    The total page views from both the platforms is obtained from the
    function below. The statistics were collected from the year
    2013-2015 hence the years are passed as parameters to the above
    function.
#+BEGIN_SRC python :tangle /analytics/extract_data.py
def total_visits():
    global __dict_tot_pages__
    years = [ '2013','2014', '2015' ]
    aws_data = r'awstats\d+.virtual-labs.ac.in.txt$'
    deploy_data = "NONE"
    tot_pages=0
    for year in years:
        #print i
        deploy_total = extract_data(year,config.deploy_stats_location,deploy_data)
       # print deploy_total
        aws_total = extract_data(year,config.aws_stats_location,aws_data)
       # print aws_total
        tot_pages += deploy_total + aws_total
      
    __dict_tot_pages__["total-pages"]=str(tot_pages)
    return __dict_tot_pages__["total-pages"]
#+END_SRC
#+BEGIN_SRC python :tangle /analytics/extract_data.py
if __name__ == '__main__':
    total = total_visits()
    print total

#+END_SRC
#+BEGIN_SRC python :tangle /analytics/config.py
aws_stats_location = '/var/www/html/aws-awstats/'
deploy_stats_location = '/var/www/html/deploy-awstats/'
#+END_SRC
*** Usage Script for Requirement1 
To convert logs to csv we use the following script:
#+BEGIN_SRC python :tangle /analytics/logs_to_csv.py 
import shlex
import os
import csv
import fnmatch
import re
import config

#__current_path__ = '/var/www/html/analytics/deploy-csv/'
def logs_to_csv(path_for_logs,path_for_csv_files):
#    global __current_path__
    files_dict = {}
    log_as_csv_files = []
    for file in os.listdir(path_for_logs):
#        print file
        if fnmatch.fnmatch(file, '*access_log*'):
            lab_id = file.split('.')[0]
            if not files_dict.has_key(lab_id):
                files_dict[lab_id] = []
                files_dict[lab_id].append(file)
            else:
                files_dict[lab_id].append(file)
    print files_dict

    for key in files_dict:
        lab_log_files = files_dict[key]
        lab_csv_file = key+'.csv'
        log_as_csv_files.append(lab_csv_file)
        with open(path_for_csv_files + lab_csv_file, 'w') as csv_file:
            for log in lab_log_files:
                with open(path_for_logs+log) as input:
                    for line in input:
                        output = ','.join(shlex.split(line)) + '\n'
                        csv_file.write(output)

if __name__ == "__main__":
    logs_to_csv(config.aws_path, config.aws_csv_path)
    logs_to_csv(config.deploy_path, config.deploy_csv_path)
#+END_SRC
We use the following script, to satisfy the [[requirement #1]]: 
#+BEGIN_SRC 
import os
import aws_config 
import deploy_config 
import config
 
sim_count = 0
ctx_count = 0
def filter(csv_path, filter_path, exp_sim_urls_for_labs):
    global sim_count 
    global ctx_count 
    usage = 0
#    files_dir = "/var/www/html/analytics/csv/"
    files_dir = csv_path
    for key in exp_sim_urls_for_labs.keys():
        file_name = "%s%s.csv" % (files_dir, key)
        pattern_list = exp_sim_urls_for_labs[key]
        file = open(file_name, 'r')
#        new_file_name = "/var/www/html/analytics/filter/filtered_%s" % key
        new_file_name = filter_path + "filtered_%s" % key
        new_file = open(new_file_name, 'w+')
        columns = []
        for line in file:
            columns = line.split(',')
            if (len(columns)>=5 and search_pattern(columns[5], pattern_list)):
                new_file.write(columns[5] + "\n")
        new_file.write("Simulation count = %d" % sim_count)
        new_file.write("Content count = %d" % (ctx_count - sim_count))
        
        if (ctx_count >= sim_count):
            usage += sim_count
        
        ctx_count = 0
        sim_count = 0

    return usage
#+END_SRC

This function will calculate the usage by taking the total count of
content and simulations and calculates the usage only if simulation
count is less than content count and takes the simulation count as
usage.

#+BEGIN_SRC 
def search_pattern(get_field, pattern_list):
    ret_val = False
    global ctx_count
    global sim_count
    for experiment in pattern_list:
        if experiment[0] in get_field:
            ctx_count += 1
            ret_val = True
        if experiment[1] in get_field:
            sim_count += 1
            ret_val = True

    return ret_val


def total_usage():    
    aws_usage = filter(config.aws_csv_path, config.aws_filter_path, aws_config.exp_sim_urls_for_labs)
    deploy_usage = filter(config.deploy_csv_path, config.deploy_filter_path, deploy_config.exp_sim_urls_for_labs)
    tot_usage = aws_usage + deploy_usage 
    print "Usage = %s" % tot_usage
    return str(tot_usage)

if __name__ == "__main__":
    usage = total_usage()
    print "Usage = %s" % usage

#+END_SRC

The config file:
#+BEGIN_SRC python :tangle /analytics/config.py
aws_path = '/var/www/html/aws-logs/'
deploy_path = '/var/www/html/deploy-logs/'

deploy_csv_path = "/var/www/html/analytics/deploy-csv/"
aws_csv_path = "/var/www/html/analytics/aws-csv/"
#+END_SRC
The follwing AWS data/pattern file contains a dictionary with =lab_id= as
its key and a list of lists with experiment and simulation pattern as
value of a particular lab. The actual AWS pattern file can be found [[./config/aws_config.py][here]]

The following example shows the sample AWS pattern file.
#+BEGIN_EXAMPLE
exp_sim_urls_for_labs = { 
'cse01-iiith':[['/exp1/index.php','/exp1/index.php?section=Simulation'],
               ['/exp2/index.php','/exp2/index.php?section=Simulation']]	
}
#+END_EXAMPLE
From the above example, 
+ =exp_sim_urls_for_labs= is the Name of the dictionary 
+ =cse01-iiith= is the lab_id which is used as a key
+ =/exp1/index.php= is the exp1 pattern of
  =cse01-iiith= lab.
+ =/exp1/index.php?section=Simulation= is the simulation pattern of
  exp1 of the =cse01-iiith= lab.
+ similarly all other experiments patterns of the same lab are
  collected as list of lists and taken as its value.


The follwing deploy data file contains a dictionary with =lab_id= as
its key and a list of lists with experiment and simulation pattern as
value of a particular lab. The actual deploy pattern file can be found [[./config/deploy_config.py][here]]

The following example shows the sample deploy pattern file.
#+BEGIN_EXAMPLE
exp_sim_urls_for_labs = {"cse18":[["/labs/cse18/exp1/index.php", "/labs/cse18/exp1/index.php?section=Experiment"],
                                  ["/labs/cse18/exp2/index.php", "/labs/cse18/exp2/index.php?section=Experiment"]]
}
#+END_EXAMPLE
From the above example, 
+ =exp_sim_urls_for_labs= is the Name of the dictionary 
+ =cse18= is the lab_id which is used as a key
+ =/labs/cse18/exp1/index.php= is the exp1 pattern of
  =cse18= lab.
+ =/labs/cse18/exp1/index.php?section=Experiment= is the 
  simulation pattern of exp1 the =cse18= lab.
+ similarly all other experiments patterns of the same lab are
  collected as list of lists and taken as its value.

*** Usage for Requirement2
    The follwoing steps will describe how to calculate the total usage for requirement 2 
      + Convert the apache logs into csv format and save them as csv
        files.
      + From the csv file, the tuples with the experiment urls need to
        be extracted.
      + A dictionary which contains the lab-ids and the experiment
        URL along with the simulation pattern is maintained in a
        separate config file.
      + From the list of tuples of a particular experiment given in the config file, a pattern
        which specifies the simulation, content will be searched  in the logs if found,
	then its ip address is also compared and if the ip address of both content and simulation
	is same then it is considered was usage. 
      + The usage is taken from several such log files and the total usage
        is calculated across all the labs and all the experiments.

The following usage script will satisfy the Requirement2.
Import the required files and packages used by this script.
#+BEGIN_SRC python :tangle analytics/usage.py
import os
from deploy_data import exp_sim_urls_for_labs 
from aws_data import exp_sim_urls_for_labs
import deploy_config
import aws_config
import config
import os.path
#+END_SRC
The following =usage()= will take the csv files as input and processes files against the patterns given in 
=exp_sim_urls_for_labs= and calculates and returns the =total_count= as usage.
#+BEGIN_SRC python :tangle analytics/usage.py
def usage(csv_path, exp_sim_urls_for_labs): # this function will take the csv file path and a dictionary which has experiment and simulation patterns
    tot_count =0
    files_dir = csv_path
    for key in exp_sim_urls_for_labs.keys():# It loops over each key in the given pattern dictionary.Here key is nothing but lab_id.
        try: 
            lab_csv_file=open(files_dir+key+".csv","r")# opens the lab csv file in read mode if available.
            lines = lab_csv_file.readlines() # once the file is opened it reads all the line one by one.
            pass
        except IOError as e:
            print "No csv file found for lab =" + key # if there is no csv file available, then it raises this exception
        
        exp_sim_list= exp_sim_urls_for_labs[key] #It will load the exp and sim patterns of a particular lab.
        cont_ip = "ip1"
        sim_ip = "ip2"
        cont_flag=0
        sim_flag=0
        count = 0
  	for line in lines: # It loops over each line in the lines.
            columns=line.split(",") # columns will have all the columns after splitting with comma.
            get_column=columns[5].split(" ")# columns[5] will have the GET /url column, by splitting with space we get only url.
            ip_urls =[]
            ip_urls.append(columns[0])#column[0] will have the ip address of the user, so we are appending them to ip_urls array.
            ip_urls.append(get_column[1]) # get_column[1] will have only the url, which is separated from GET.	
            for i in range(len(exp_sim_list)): # It will loop over the length of exp_sim_list 
                if (exp_sim_list[i][0] == ip_urls[1]): # exp_sim_list[i][0] will have either the exp_url which is compared with ip_urls[1] which has url from log.
                   # print ip_urls
                    cont_ip = ip_urls[0] # once content url is found, ip of that url is stored in cont_ip and cont_flag is set to 1.
                    cont_flag = 1 
              
                if(exp_sim_list[i][1] == ip_urls[1]):# exp_sim_list[i][1] will have either the sim_url which is compared with ip_urls[1] which has url from log.
                    print ip_urls
                    sim_ip = ip_urls[0] # once simulation url is found, ip of that url is stored in sim_ip and sim_flag is set to 1.
                    sim_flag = 1
                if(cont_flag==1 and sim_flag==1 and cont_ip == sim_ip): # once the content and simulation urls are found, it will compare the ip address.
                   cont_flag=0
                   sim_flag=0
                   count += 1 # if cont_ip and sim_ip are equal then it will increment the count.
                   print count
        #print key," = ", count
                
        tot_count += count # to give the total usage count, each lab counts are added simultanously.

    return tot_count
#+END_SRC
the following =total_usage()= will call the =usage()= twice, one for the AWS and other for Deploy labs.
Adds the usage of AWS and Deploy and returns the total usage.
#+BEGIN_SRC python :tangle analytics/usage.py		
def total_usage():    
    aws_usage = usage(config.aws_csv_path, aws_config.exp_sim_urls_for_labs)
    deploy_usage = usage(config.deploy_csv_path, deploy_config.exp_sim_urls_for_labs)
    print aws_usage
    print deploy_usage
    tot_usage = aws_usage + deploy_usage 
    print "Usage = %s" % tot_usage
    return str(tot_usage)
#+END_SRC
The main method below will make a call to =total_usage()= to find the total usage of all labs.
#+BEGIN_SRC python :tangle analytics/usage.py
if __name__ == '__main__':
    usage = total_usage()
    print "Usage = %s" % usage
#+END_SRC
Adding usage script as a package in =__init__.py= file.
#+BEGIN_SRC python :tangle analytics/__init__.py 
import usage
#+END_SRC
** Building the Service
   + For building the service, python-flask is used.
   Flask is a micro web application framework written in Python.
   more details about flask can be found [[http://flask.pocoo.org/docs/0.10/][here]].

   + This service, in its present state, will parse the data, extract
   aws-stats, calculate and return the total number of page visits.
   It exposes one endpoint =/numberofhits=.  An "endpoint" is an 
   identifier that is used in determining what logical unit of code
   should handle the request. 

#+BEGIN_SRC python :tangle analytics/app.py
from flask import Flask, render_template, request, jsonify, make_response
from flask.ext.cache import Cache
import json
from results import *
#+END_SRC

Create an instance of the class(Flask). The argument is the name of
the application’s module or package.  This is needed so that Flask
knows where to look for templates, static files, and so on.

#+BEGIN_SRC python :tangle analytics/app.py
app = Flask(__name__)
#+END_SRC

  Define the cache config keys.

#+BEGIN_SRC python :tangle analytics/app.py
app.config['CACHE_TYPE'] = 'simple'
#+END_SRC

  Register the cache instance and bind it to app. 

#+BEGIN_SRC python :tangle analytics/app.py
app.cache = Cache(app)
#+END_SRC

  Register the cache instance and bind it to app. 

Use the =route()= decorator to tell Flask what URL should trigger the
function.

#+BEGIN_SRC python :tangle analytics/app.py
@app.route('/numberofhits')
#+END_SRC

 Cache this view for 1 hour. This is needed because the service must
 display the updated count every one hour.

#+BEGIN_SRC python :tangle analytics/app.py
@app.cache.cached(timeout=360)  
#+END_SRC

 The function is given a name which is also used to generate URLs
 for that particular function, and returns the value we want to
 display in the browser.
 
#+BEGIN_SRC python :tangle analytics/app.py
def pageviewhits():
#+END_SRC

 get the total visits from the =visits.txt= file
#+BEGIN_SRC python :tangle analytics/app.py
    files_dir = "/var/www/html/analytics/"
    result_file = files_dir+"visits.txt"
    with open(result_file, 'r') as f:
        visits = f.read()
#+END_SRC

   The =make_response()= function can be called instead of using a return
   and will get a response object which can be used to attach headers.
   =Access-Control-Allow-Origin= is a CORS (Cross-Origin Resource
   Sharing) header.  When Site A tries to fetch content from Site B,
   Site B can send an =Access-Control-Allow-Origin= response header to
   tell the browser that the content of this page is accessible to
   certain origins.  By default, Site B's pages are not accessible to
   any other origin, using the =Access-Control-Allow-Origin= header
   opens a door for cross-origin access by specific requesting
   origins.  The server can give permission to include cookies by
   setting the =Access-Control-Allow-Credentials= header.

#+BEGIN_SRC python :tangle analytics/app.py
    response = make_response(visits)
    response.headers['Access-Control-Allow-Origin'] = '*'
    response.headers['Access-Control-Allow-Credentials'] = 'true'
    return response
#+END_SRC
   Similary, usage count will get from the following end point.
#+BEGIN_SRC python :tangle analytics/app.py
@app.route('/usage')
@app.cache.cached(timeout=360)  
def usagehits():
    files_dir = "/var/www/html/analytics/"
    result_file = files_dir+"usage.txt"
    with open(result_file, 'r') as f:
        usage = f.read()
    response = make_response(usage)
    response.headers['Access-Control-Allow-Origin'] = '*'
    response.headers['Access-Control-Allow-Credentials'] = 'true'
    return response
#+END_SRC

   Finally, use the =run()= function to run the local server with
   our application. The if *__name__ == '__main__':* makes sure the
   server only runs if the script is executed directly from the Python
   interpreter and not used as an imported module.  

#+BEGIN_SRC python :tangle analytics/app.py
if __name__ == '__main__':
    app.run(port=5000, debug=True)
#+END_SRC
The following =results.py= is used to make a call to the scripts and get
the calculated values and store them in a separate file. So that rendering
of the endpoints will take less time. 
#+BEGIN_SRC python :tangle analytics/results.py
#!/usr/bin/python
from usage import *
from extract_data import *
#+END_SRC
 Call the function =total_visits()= which is defined in
 =extract_data.py= to get the total number of hits.
 and write to =visits.txt= file.
#+BEGIN_SRC python :tangle analytics/results.py
def tot_visits():
   result_file = open("visits.txt", 'w')
   visits = total_visits()
   result_file.write(visits)
   return visits
#+END_SRC
Call the function =total_usage()= which is defined in
 =usage.py= to get the total usage.
 and write to =usage.txt= file.

#+BEGIN_SRC python :tangle analytics/results.py
def tot_usage():
   result_file = open("usage.txt", 'w')
   usage = total_usage()
   result_file.write(usage)
   return usage
#+END_SRC
#+BEGIN_SRC python :tangle analytics/results.py
if __name__ == '__main__':
   tot_visits()
   tot_usage()

#+END_SRC
**** Create a =.wsgi= file
    To run your application you need a =analytics.wsgi= file. 
    This file contains the code =mod_wsgi= is executing on startup to get the application object. 
    The object called application in that file is then used as application.
#+BEGIN_SRC  python :tangle analytics/analytics.wsgi
import sys
sys.path.insert (0,'/var/www/html/analytics/')

import logging, sys
logging.basicConfig(stream=sys.stderr)

from app import app as application
 #+END_SRC
** Presenting the data
   For presenting the data, AJAX (Asynchronous JavaScript and
   XML)and HTML(HyperText Markup Language) is used. 
   AJAX allows web pages to be updated asynchronously by
   exchanging small amounts of data with the server. This means that
   it is possible to update parts of a web page, without reloading the
   whole page. The code snippet below will call the endpoint
   =/numberofhits= and =/usage=, where it will return the total number of pages 
   visited and usage as a response and displays on browser.

#+BEGIN_SRC 
<html>
   <head>
      <title>Analytics</title>
      <script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
      <script type="text/javascript" language="javascript">
         //$(document).ready(function() {
         window.onload = function() {
               $.getJSON('http://stats-demo.vlabs.ac.in/numberofhits', function(data) {
                  $("#analytics").html("<p><b>Page hits since 2013: " + data + "</b></p>");
                  });            
                  $.getJSON('http://stats-demo.vlabs.ac.in/usagehits', function(data) {
                  $("#usage").html("<p><b>Usage hits since 2013: " + data + "</b></p>");

              });
         };
      </script>
     </head>
   <body>
      <div id="analytics">
         Analytics
      </div>
      <div id="usage">
         Usage
      </div>
   </body>
</html>
#+END_SRC

   Note: If you want to display the total number of page visits and usage on vlab.co.in, 
   then add the above code snippet in =index.html= file. 
* Test Cases
  A test case is a document, which has a set of test data,
  preconditions, expected results and post-conditions, developed for a
  particular test scenario in order to verify compliance against a
  specific requirement.
** Objective:
   The objective of this test cases is to test whether the application 
   is running or not, by sending HTTP GET request and also to test 
   the return value of the response. 
** Test Case ID: TC01
*** Test case name: Test Number of Page Visits
*** Test case description:
    In this test case, end point =/numberofhits= is tested, where it 
    will return the total number of pages visited.
*** Test data/Input data:
   The Input data is required to test the test case. 
   Input data is to verify that a given set of 
   input to a given function/program produces some expected result. 
   Input can be valid data or invalid data.
   Here, the input data will return the user-defined value when
    =grand_total()= function is called. 
#+BEGIN_SRC python :tangle analytics/tests/sample_data.py
#!/usr/bin/python
def grand_total():
  dict = {'numberofhits':'1234'};
  return dict['numberofhits']
#+END_SRC
*** Step description/action
**** Step 1:
    Import the required packages used for testing.
#+BEGIN_SRC python :tangle analytics/tests/test_app.py
import unittest
from flask.ext.testing import TestCase
import sample_data
from app import app
#+END_SRC
**** Step 2:
 A test case is created by sub-classing =unittest.TestCase=.
#+BEGIN_SRC python :tangle analytics/tests/test_app.py
class AnalyticsTestCase(unittest.TestCase):
#+END_SRC
**** Step 3:
    The tests are defined with methods whose names start with the
    letters =test=.  This naming convention informs the test runner
    about which methods represent tests.  To start testing the
    functionality of the application, add a new test method to our
    class, like =test_numberofhits()=.Similarly, other test methods
    can be added. 
#+BEGIN_SRC python :tangle analytics/tests/test_app.py 
    def test_numberofhits(self):
        json_string = sample_data.grand_total()
        print json_string
#+END_SRC
**** Step 4:
   Each test is a call to =assertEqual()= function which will test
   that first and second arguments are equal.  If the values do not
   compare equal, then test will fail.
   - The status code of the endpoint =/numberofhits= is equated to =200= 
   which means =OK= (the request has succeeded). The endpoint returned 
   with the response.
   - The data which is returned in the form of response is compared against
     the Input data.
#+BEGIN_SRC python :tangle analytics/tests/test_app.py 
        tester = app.test_client(self)
        response = tester.get('http://localhost:5000/numberofhits')
        self.assertEqual(response.status_code, 200)
        self.assertEqual(json_string,  response.data)

#+END_SRC            
**** Step 5:
    The final block shows a simple way to run the
    tests. =unittest.main()= provides a command-line interface to the
    test script.  When run from the command line, the script
    produces an output.
#+BEGIN_SRC  
if __name__ == '__main__':
    unittest.main()
#+END_SRC

*** Expected Result
    - Run =python test_app.py= in terminal.
    - Check the output, if it was like =Ran 1 test in 0.034s= and =OK=.
    - If the output is found then test is passed.
 #+BEGIN_SRC 
$ python test_app.py
.
----------------------------------------------------------------------
Ran 1 test in 0.034s

OK
#+END_SRC

** Test Case ID: TC02
*** Test case name: Test Usage for R2 
*** Test case description:
    In this test case, the functionality of =usage()= is tested by known usage,
    where it will return the total number of usage.
*** Test data/Input data:
   The Input data is required to test the test case.  Input data is to
   verify that a given set of input to a given function/program
   produces some expected result. 

   The input data to the test case is the sample csv file with
   apache access logs (separated by comma)of [[./tests/test-deploy-csv/test_cse01.csv][cse01]] lab from deploy and
   [[./tests/test-aws-csv/test_cse01-iiith.csv][cse01-iiith]] lab from AWS. 

  The usage value returned from the test case can be verified against
   the user-defined value.

#+BEGIN_SRC python :tangle analytics/tests/sample_data.py 
def aws_usage():
  dict = {'aws_usage':2};
  return dict['aws_usage']
def deploy_usage():
  dict = {'deploy_usage': 1};
  return dict['deploy_usage']
#+END_SRC
*** Step description/action
**** Step 1:
    Import the required packages used for testing.
#+BEGIN_SRC python :tangle analytics/tests/test_usage.py
import unittest
from flask.ext.testing import TestCase
import sample_data
import test_deploy_config 
import test_aws_config
import config 
from usage import usage
#+END_SRC
**** Step 2:
 A test case is created by sub-classing =unittest.TestCase=.
#+BEGIN_SRC python :tangle analytics/tests/test_usage.py
class AnalyticsTestCase(unittest.TestCase):
#+END_SRC
**** Step 3:
    The tests are defined with methods whose names start with the
    letters =test=.  This naming convention informs the test runner
    about which methods represent tests.  To start testing the
    functionality of the application, add a new test method to our
    class, like =test_usage()=.Similarly, other test methods
    can be added. 
#+BEGIN_SRC python :tangle analytics/tests/test_usage.py 
    def test_usage_r2(self):
       aws_expected_usage = sample_data.aws_usage()
       deploy_expected_usage = sample_data.deploy_usage()
       aws_usage = usage(config.aws_csv_path, test_aws_config.exp_sim_urls_for_labs)
       deploy_usage = usage(config.deploy_csv_path, test_deploy_config.exp_sim_urls_for_labs)
       self.assertEqual(aws_expected_usage,  aws_usage)
       self.assertEqual(deploy_expected_usage,  deploy_usage)
#+END_SRC
**** Step 4:
   Each test is a call to =assertEqual()= function which will test
   that first and second arguments are equal.  If the values do not
   compare equal, then test will fail.
**** Step 5:
    The final block shows a simple way to run the
    tests. =unittest.main()= provides a command-line interface to the
    test script.  When run from the command line, the script
    produces an output.
#+BEGIN_SRC python :tangle analytics/tests/test_usage.py
if __name__ == '__main__':
    unittest.main()
#+END_SRC

*** Expected Result
    - Run =python test_app.py= in terminal.
    - Check the output, if it was like =Ran 1 test in 0.034s= and =OK=.
    - If the output is found then test is passed.
 #+BEGIN_SRC 
.
----------------------------------------------------------------------
Ran 1 test in 0.000s

OK
#+END_SRC

* Releases
** Release v1.0.0
   This release realizes [[Requirement #1]].
   The release date is [2015-07-08 Wed]
*** Work Plan
****  Prepare the document
****  Discuss on how to meet the requirement
          There was a discussion on [2015-06-23 Tue] with the
          participants as Saurabh, Thirumal, Anon, Zubair, Soumya and
          the other VLEAD members. The task division and the process to
          bring up the service was also discussed which is mentioned in
          the [[Process%20for%20implementation][Process for implementation]] section.
****  Figure out the various location of the labs
          There are several locations the labs are situated namely AWS,
          deploy, separate containers. How to display the combined
          output of all the labs is yet to be decided.  (This has been
          done and the decision taken has been mentioned in the [[Present%20scenario%20of%20statistics][Present
          scenario of statistics]])

****  Build the scripts
****  Test the scripts.
****  Updation of page view count in new landing page of vlab.co.in

** Release v1.0.1
   This release realizes [[Requirement%20#2][Requirement #2]].
   The release date is: [2015-08-01 Sat]
*** Work Plan
****  Update model with usage design [[https://github.com/vlead/analytics/issues/2][#2]]
****  Write script  to convert logs into csv format [[https://github.com/vlead/analytics/issues/9][#9]]
****  Write a python script to extract usage form the CSV formatted logs. [[https://github.com/vlead/analytics/issues/8][#8]]
****  Update model with usage extraction script implementation [[https://github.com/vlead/analytics/issues/12][#12]]
****  Add a new endpoint to the service [[https://github.com/vlead/analytics/issues/6][#6]]
****  A display page to show the usage [[https://github.com/vlead/analytics/issues/7][#7]]
****  Update model with html, ajax and js implementation [[https://github.com/vlead/analytics/issues/13][#13]]
****  Update model with service implementation [[https://github.com/vlead/analytics/issues/3][#3]]
****  Write test cases [[https://github.com/vlead/analytics/issues/5][#5]]
****  Update model with test-cases [[https://github.com/vlead/analytics/issues/4][#4]]
****  Add usage definitions to pop-up page [[https://github.com/vlead/analytics/issues/14][#14]]
****  Add usage definitions to pop-up page [[https://github.com/vlead/analytics/issues/14][#14]]

** Release v1.0.2
   This release realizes [[Requirement #2]].
   The release date is: [2015-09-23 Wed]
*** Work Plan
  Status of work can be found from the following tasks:
|------+------------------------------------+------------+------------+------------|
| S.No | Tasks                              | No.of Days | Start Date | End Date   |
|------+------------------------------------+------------+------------+------------|
|    1 | Analytics Service Model-Design     |          2 | 21/09/2015 | 23/09/2015 |
|------+------------------------------------+------------+------------+------------|
|    2 | Implementation of Requirement - #2 |          3 | 10/09/2015 | 14/09/2015 |
|------+------------------------------------+------------+------------+------------|
|    3 | Testing                            |          1 | 15/09/2015 | 15/09/2015 |
|------+------------------------------------+------------+------------+------------|
|    4 | Model Review                       |          1 | 16/09/2015 | 16/09/2015 |
|------+------------------------------------+------------+------------+------------|
|    5 | Review Changes                     |          1 | 18/09/2015 | 18/09/2015 |
|------+------------------------------------+------------+------------+------------|
