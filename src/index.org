#+TITLE:     Building a basic service to display the Analytics of Virtual Labs
#+AUTHOR:    M.S.Soumya
#+DATE:      2015-06-09 Tue
#+PROPERTY: results output
#+PROPERTY: exports code
#+SETUPFILE: org-templates/level-0.org

*NOTE:* This document is not yet complete though it is committed ! It
is being worked on continuously.

* Problem
  The analytics of Virtual-labs are currently generated by
  awstats. Awstats provides a dashboard for viewing these statistics.
  It provides a layman with too much information to comprehend.  A
  simple dashboard which displays the total views and hits of the site
  is required. This would help keep us informed on the usage of our
  labs.

* Objective
  The objective of this exercise is to display the usage of
  virtual-labs in terms of number of pages viewed on vlab.co.in.

* Plan
  #+CAPTION:  Design diagram
  #+LABEL:  Design diagram
  [[./diagrams/analytics.png]]
 
  To build a web service which would display certain specific
  information from the statistics generated by awstats. As the diagram
  suggests, this service would have a URL which would be hyper-linked
  to a button on the vlab.co.in page.  On clicking the button this
  service would get invoked and the information is displayed on the
  page. This would be a dynamic process and any changes in the
  statistics must be incorporated by the service.

* Implementation
** Requirement
   The pages views count from both AWS and local deployment must be
   displayed on vlab.co.in
** Background
   Awstats generates parsed text files which contain the statistics in
   text format. The required data from these text files needs to be
   extracted and displayed on a HTML page.To achieve this we build the
   service.  

   *Note*: Currently the HTML page is on vlab.co.in home page. In
   future a button / hyperlink could also be incorporated which would
   display a page that shows slightly more details.

** Present status of statistics
   Currently the labs are hosted in 3 different locations namely:
   + AWS ( Currently 63 labs)
   + Containers at VLEAD (Around 20 labs) 
   + Rest of the labs directed by vlab.co.in (either hosted by Amrita
     or individual institutes)
   
   The statistics of the labs hosted on AWS and the containers at
   VLEAD can be obtained because Awstats was configured and the
   statistics file are accessible. The statistics of the rest of the
   labs which are directed by vlab.co.in (not on AWS or on the VLEAD
   containers) cannot be obtained.

** Decisions
*** Collation of statistics
    The statistics are present in two different locations AWS and on
    the local reverseproxy at VLEAD. To get numbers the data is
    required from both these locations. It was decided that on an AWS
    VM the data from both locations would be collated. The awstats
    data will be transferred on an hourly basis to this VM. 

*** Where the service will run
    There were concerns on where the service should run. Initially it
    was being assumed that the service would run on the reverse proxy
    server itself. This was so because the stats file were located on
    this machine. By doing so it would reduce the overhead of
    transferring the stats files into another location and also the
    data that the service would use might be stale.  Setting up the
    service on the reverse proxy had its own security threats.  So,
    after a lot of discussion it was decided that the service will run
    on a separate VM on AWS.
*** Setup of the service
    As discussed in the [[Where%20the%20service%20will%20run][Where the service will run]], a VM on AWS will
    be created to run the service. The VM will be a part of the AWS
    cluster. The concern in setting up this VM is that the other VM's
    in the cluster are setup by ansible scripts. To setup this service
    ansible scripts need to be written. Manual entry of the FQDN and
    IP into some servers like reverse proxy and DNS may work
    temporarily but when the scripts are run, this information will be
    erased.  *Note*: Yet to be discussed.
** Procedure
   After a discussion (which also involved exchange of mails) the
   process of implementation was decided as follows:
   1. Pull all the data from deploy and AWS
      - Collate data from deploy and AWS
      - This data that will be used by the service using helper functions.
   2. Serving the data. 
      - Define end points based on the requirements
      - Build a service that implements these end points.
   3. Rendering of the data on vlab.co.in
      - Insert the JavaScript snippet in vlab.co.in
   4. Backup of all the statistics
      
   #+CAPTION:  Implementation diagram
   #+LABEL:  Implementation diagram
   [[./diagrams/file-flow-diagram.png]]

** Impediments

** Data collation

   We have two different sources, a reverse proxy on AWS
cluster(Source 1) and a reverse proxy on the base machines in IIIT-H
(Source 2). These sources are geographically located in different
continents and are on different networks.

   We need a way of transferring the data(statistics) to the analytics
server where it will be processed. We have used =rsync= to transfer,
and have setup periodic transfer jobs using =cronie= which executes
every hour.

As this task involves collecting data from different sources, like a
cluster which is managed with configuration management tool =Ansible=
and another one a manually configured server, data collation is also a
mix of automation and manual steps.

We now present the configuration procedure of both.

*** Configuring AWS cluster
 Here we need the following:
 - Analytics server configured to accept =rsync= over =TCP=
 - Reverse Proxy(Source 1) configured to push the generated statistics
   to =Analytics= server at regular intervals
 - Router cofigured to allow incoming =rsync= connections from Source
   2 to Analytics server
 
 As AWS cluster is configured by =Ansible=, we have scripts to achieve
 the above via =Ansible=.

*** On BASE
 Here we manually setup:
 - Reverse Proxy(Source 2) configued to push generated statistics to
   =Analytics= server every hour

*** TODO
 -  Find a way to rsync after/before awstats update script, so the
    chances of file corruption are minimized.

** Data extraction
   The code below is written to extract certain fields from the
   awstats text files.
#+BEGIN_SRC python :tangle extract_data.py
import sys
import re
import json
import os
import fnmatch

def get_file_names(year):
    list_of_files = []
    for file in os.listdir('.'):
        if fnmatch.fnmatch(file, '*'+ re.escape(year) + '*.txt'):
            list_of_files.append(file)
    return list_of_files

def extract_aws_data(year):
    files = get_file_names(year);
    dictionary = {}
    dict_tot_pages = {}
    for i in files:
        f = open(i)
        for line in f:
            if re.match(r'^BEGIN_DAY',line):
                n = int((line.strip('BEGIN_DAY ')).rstrip())
                x = 0
                p = 0
                for page_line in f:
                    if (x >= n):
                        break
                    p += int(page_line.split()[1])
                    x += 1
                f.close()
                dictionary[(i.lstrip('awstats.')).rstrip('.txt')] = p
                break
    z = 0
    for key in dictionary:
        z += dictionary[key]
#    print "total aws",z
    return z

def extract_iiit_data(year):
    files = get_file_names(year);
    dictionary = {}
    dict_tot_pages = {}
    for i in files:
        if re.search(r'awstats\d+.virtual-labs.ac.in.txt$',i):
            print i
            f = open(i)
            for line in f:
                if re.match(r'^BEGIN_DAY',line):
                    n = int((line.strip('BEGIN_DAY ')).rstrip())
                    x = 0
                    p = 0
                    for page_line in f:
                        if (x >= n):
                            break
                        p += int(page_line.split()[1])
                        x += 1
                    f.close()
                    dictionary[(i.lstrip('awstats.')).rstrip('.txt')] = p
                    break
    z = 0
    for key in dictionary:
        z += dictionary[key]
#    print "total iiit",z
    return z

def grand_total():
    year = ['2013', '2014', '2015' ]
    tot_pages=0
    for i in year:
        tot_pages += extract_iiit_data(i) + extract_aws_data(i)
    dict_tot_pages = {}
    dict_tot_pages["total-pages"]=tot_pages
#    print dict_tot_pages

grand_total()
#+END_SRC

** Display of data
 Following code dipicts the implementation of python-flask application as a service with endpoint =/numberofhits=. 
 this code will parse the data and extracts aws-stats and calculates the total numer of visits and return the value.
#+BEGIN_SRC python :tangle app.py
#flask imports
from flask import Flask, render_template, request, jsonify, make_response
import json
import os
#import config
from data import extract_data

# import the flask extension
from flask.ext.cache import Cache

app = Flask(__name__)

# define the cache config keys
app.config['CACHE_TYPE'] = 'simple'

# register the cache instance and binds it on to your app 
app.cache = Cache(app)

@app.route('/numberofhits')
@app.cache.cached(timeout=60)  # cache this view for 1 minute
def numberofhits():
    #curr_dir = os.path.dirname(os.path.realpath(__file__))
    #json_data = open(os.path.join(curr_dir, "data-2014.json"))
    #data = json.load(json_data)
    #json_string = json.dumps(data)
    numberofhits  = extract_data.grand_total()
    response = make_response(numberofhits)
    response.headers['Access-Control-Allow-Origin'] = '*'
    response.headers['Access-Control-Allow-Credentials'] = 'true'
    return response
    #return jsonify(data=json_string)

if __name__ == '__main__':
    app.run(port=5000, debug=True)
#+END_SRC


* Work Plan
** TODO Prepare the document
** DONE Discuss on how to meet the requirement
        There was a discussion on [2015-06-23 Tue] with the
        participants as Saurabh, Thirumal, Anon, Zubair, Soumya and
        the other VLEAD members. The task division and the process to
        bring up the service was also discussed which is mentioned in
        the [[Process%20for%20implementation][Process for implementation]] section.
** DONE Figure out the various location of the labs
        There are several locations the labs are situated namely AWS,
        deploy, separate containers. How to display the combined
        output of all the labs is yet to be decided.  (This has been
        done and the decision taken has been mentioned in the [[Present%20scenario%20of%20statistics][Present
        scenario of statistics]])

** TODO Build the scripts
** TODO Test the scripts.
** TODO Updation of vlab.co.in
   
