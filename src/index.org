#+TITLE:     Building an application for the Analytics of Virtual Labs
#+AUTHOR:    M.S.Soumya
#+DATE:      2015-06-09 Tue
#+PROPERTY: results output
#+PROPERTY: exports code
#+SETUPFILE: org-templates/level-0.org

* Introduction
  Web analytics is used to measure the web traffic which helps in
  knowing the site visitor count and several others values like the
  number of page views, resources downloaded etc. This model describes
  a simple web application which displays the analytics of
  virtual-labs.
  
* Requirements
** Requirement #1
   1. The "visitor page views" count of the labs deployed on Amazon
      web services(AWS) and IIIT infrastructure from both AWS and
      local deployment must be displayed on vlab.co.in
   2. The "visitor page views" count must be updated every one hour.
** Requirement #2
   Display the total =usage= of virtual-labs. 
    - Definition of =usage= :: 1 usage (instance) is loading content
         and simulation of one experiment.

** Requirement #3
   Display the usage at an experiment level.
   
** Requirement #4
   Display the lab-wise analytics.

* Design
** Design description
   This model builds a web service that provides endpoints to get
   different sets of information from the statistics generated by
   awstats and apache logs. A four tier architecture is built with the
   following components :
   - data : the specific data based on the requirement.
   - service : data processing and service to provide data
   - view : presentation of data.
   The tiers in the architecture are :
   + Collation of data
   + Extraction of data
   + Service to provide data
   + Presentation
   
   The design of the architecture is depicted in the diagram below.
   
   #+CAPTION:  Design diagram
   #+LABEL:  Design diagram
   [[./diagrams/analytics.png]]

   The analytics service as shown in the above diagram has inputs
   coming from two sources. The first source is from the log files
   generated by the apache web server. These are used for realizing
   [[Requirement%20#2][requirement #2]]. The other input to the service is the awstats text
   files. This is used to realize [[Requirement%20#1][requirement #1]].

*** Collation of data
    Initially all the labs were hosted from VLEAD infrastructure.  For
    scalability the hosting of labs shifted to Amazon Web Services
    (AWS). The statistics for labs deployed on both these platforms
    were collected. Later the deploy container was decommissioned and
    certain labs were moved to individual containers. More details
    about the deploy decommissioning project can be found [[https://bitbucket.org/vlead/vlead/src/8849aa7d1a44b25dee4ac1b88f4c5f4327e8f2a1/projects/cleaning-deploy/?at%3Dmaster][here]].

    Currently the labs are hosted in 3 different platforms namely:
    + AWS (Currently 63 labs)
    + Containers at VLEAD (Around 20 labs) 
    + Rest of the labs directed by vlab.co.in (either hosted by
      Amrita or individual institutes)
    
    The statistics of the labs hosted on AWS and the containers at
    VLEAD are obtained because VLEAD has configured awstats on both
    these platforms.  The statistics of the rest of the labs which are
    directed by vlab.co.in (not on AWS or on the VLEAD containers) are
    not obtained.

    #+CAPTION:  data collation design diagram
    #+LABEL:  data collation design diagram
    [[./diagrams/file-flow-diagram.png]] 

    The diagram above shows the two platforms and how the data is
    collated from them into a single VM. The required files from the
    reverse proxy on both the platforms is transferred to the VM
    running the analytics service every one hour.

*** Extraction of data
    During lab usage all the traffic ends up at the reverse
    proxy. Awstats is configured on the reverse proxy. All the lab
    usage data is captured by the reverse proxy in the form of logs.

**** How data is generated
     Awstats is a log analyzer which parses the logs and generates the
     analytics.  Since the logs are stored in the reverse proxy,
     awstats is installed on the same machine.  Awstats generates text
     files which contain the statistics in text format.  For each lab,
     a new gets generated each month.  The daily statistics are
     updated in the monthly text file.  The required data from these
     text files needs to be extracted. Also the access log of the web
     server (apache) are required to extract certain data.
**** How the data is processed
     To extract the required information scripts are written.  For
     [[Requirement%20#1][requirement #1]] the script does the following:
      + parse the awstats data files
      + extract the =number of pages viewed= from each data file
      + Sum up all such figures from all the files in AWS
      + Sum up all such figures from the specific files in VLEAD
      + Total the number of pages viewed from AWS and VLEAD.
     For [[Requirement%20#2][requirement #2]] the script does the following:
      + Convert the apache logs into csv format and save them as csv
        files.
      + From the csv file, the tuples with the experiment urls need to
        be extracted.
      + From the list of tuples of a particular experiment, a pattern
        which specifies that a simulation was loaded (simulation
        pattern) is found and a separate list is created.
      + A toggle variable is set to decide while parsing the
        experiment list to decide whether flow is in the form of a
        usage or not.
      + A dictionary which contains the lab-ids and the experiment
        URL along with the simulation pattern is maintained in a
        separate config file.
      + The usage is taken from several such log files and the total
        is calculated across all the labs and all the experiments.

*** Service to provide data
    A service exposes different endpoints. Currently the endpoint that
    is exposed provides total number of pages. To provide this data
    the service invokes the logic built into the data extraction.
*** Presentation
    To display the endpoints of the service scripts are required which
    fetches the data from the service.

** Design decisions
*** Collation of statistics
    The statistics are present in two different locations, Amazon Web
    Services (AWS) and on the reverse proxy at VLEAD. To get numbers
    the data is required from both these locations. It was decided
    that on an AWS VM the data from both locations would be
    collated. The awstats data from both platforms will be transferred
    on an hourly basis to this VM.
*** Where the service will run
    There were concerns on where the service should run. Initially it
    was being assumed that the service would run on the reverse proxy
    server itself. This was so because the stats file were located on
    this machine. By doing so it would reduce the overhead of
    transferring the stats files into another location and also the
    data that the service would use might be stale.  Setting up the
    service on the reverse proxy had its own security threats.  So, it
    was decided that the service will be setup on a VM which belongs
    to the AWS cluster.
*** Setup of the service
    As mentioned in the [[Where%20the%20service%20will%20run][Where the service will run]], a VM on AWS will
    be created to run the service. The VM will be a part of the AWS
    cluster. The concern in setting up this VM is that the other VMâ€™s
    in the cluster are setup by ansible scripts. To setup this service
    ansible scripts need to be written. Manual entry of the FQDN and
    IP into some servers like reverse proxy and DNS may work
    temporarily but when the scripts are run, this information will be
    erased. To avoid this, this VM should be modelled as a node and
    added to the systems-model with the necessary modifications
    involved in setting up this VM.

*** Extraction of usage 
    There were concerns on how the usage patterns can be
    extracted. The awstats data files do not have sufficient data
    which can be used to obtain the usage count. The log files have
    useful information like the IP address, time at which the request
    was made, the type of request made, the status code, the browser
    used by the user etc., which can be used to get the usage
    count. These logs are generated by the web server (Apache).  Each
    record in the log file corresponds to a request made by the user
    to the web server. Using this information the usage count can be
    determined.

*** Collation of logs
    The log files are the key input for realizing [[Requirement%20#2][Requirement #2]],#3,
    and #4. The log files generated by Apache are stored as access
    logs in the web server. Currently there are two web servers: one
    on AWS and one on the IIIT infrastructure. The logs from both the
    locations are required and are to be collated into the same
    location where the service runs. The logs generated by the web
    servers are rotated by logrotate at the source. Based on the
    configuration of logrotate, the logs get rotated and the old logs
    get deleted after a certain period of time. 

    Current status of data: 

    *IIIT infrastructure :* Before the deploy container was
    decommissioned, all the labs hosted on IIIT infrastructure were
    running of the same container. There are separate log files for
    each lab on this container. The log files have all the logs from
    2014 January till date. The logs of all the labs which were
    installed on individual containers after the decommissioning of
    deploy are also available. There are some labs however, whose
    logs from 2013-September are available. 

    *AWS :* The labs on AWS went live from April. Initially only 15
    labs were hosted. The rest of the labs were deployed on request
    basis. The logs for April to the last week of May are not
    available. The logs of the labs are logged from the time they are
    deployed. The logs of all the labs (from May) on AWS are
    available.

* Implementation
** Data collation
   We have two different sources, a reverse proxy on AWS
   cluster(Source 1) and a reverse proxy on the base machines in IIIT-H
   (Source 2). These sources are geographically located in different
   continents and are on different networks.

   We need a way of transferring the data(statistics) to the analytics
   server where it will be processed. We have used =rsync= to transfer,
   and have setup periodic transfer jobs using =cronie= which executes
   every hour.

   As this task involves collecting data from different sources, like a
   cluster which is managed with configuration management tool =Ansible=
   and another one a manually configured server, data collation is also a
   mix of automation and manual steps.

   We now present the configuration procedure of both.

*** Re-configuring AWS cluster, requirements of analytics node
 - Analytics server configured to accept =rsync= over =TCP=
 - Reverse Proxy(Source 1) configured to push the generated statistics
   to =Analytics= server at regular intervals
 - Router cofigured to allow incoming =rsync= connections from Source
   2 to Analytics server
 
    As AWS cluster is configured by =Ansible=, we have scripts to achieve
    the above via =Ansible=.

*** On BASE
    Here we manually setup:
 - Reverse Proxy(Source 2) configued to push generated statistics to
   =Analytics= server every hour
   
** Data extraction
*** Page-hits extraction
    The purpose of the script is described in [[How%20the%20data%20is%20processed][How the data is
    processed]] for [[Requirement%20#1][requirement #1]]. The python script is as described
    below.  The necessary packages are imported and some global
    variables are declared:
#+BEGIN_SRC python :tangle analytics/extract_data.py
import sys
import re
import json
import os
import fnmatch
__dict_tot_pages__ = {}
all = [__dict_tot_pages__ ]
#+END_SRC
#+BEGIN_SRC python :tangle analytics/extract_data.py
def get_file_names(year,location):
    list_of_files = []
    for file in os.listdir(location):
        if fnmatch.fnmatch(file, '*'+ re.escape(year) + '*.txt'):
            list_of_files.append(location + file)
    return list_of_files
#+END_SRC
    There are several awstats files. Some are ".bkp" files and some
    are ".txt" files. We only require the ".txt" files. The following
    function gets all the required awstats text files from the
    specified directory.  From the data collected from AWS, the
    awstats are lab specific. In AWS, awstats is not configured to
    collect the analytics for all the labs together. So, all the
    awstats data files need to be parsed to obtain the page views. The
    function below looks through all the files and adds the page
    views. It returns the total page views of labs deployed on AWS
    infrastructure.
#+BEGIN_SRC python :tangle analytics/extract_data.py
def extract_aws_data(year,location):
    files = get_file_names(year,location);
    dictionary = {}
    dict_tot_pages = {}
    for i in files:
        f = open(i)
        for line in f:
            if re.match(r'^BEGIN_DAY',line):
                n = int((line.strip('BEGIN_DAY ')).rstrip())
                x = 0
                p = 0
                for page_line in f:
                    if (x >= n):
                        break
                    p += int(page_line.split()[1])
                    x += 1
                f.close()
                dictionary[(i.lstrip('awstats.')).rstrip('.txt')] = p
                break
    z = 0
    for key in dictionary:
        z += dictionary[key]
    return z
#+END_SRC
    Awstats on the reverse proxy is configured to capture the
    analytics of all the labs at a single place. These files are named
    as =awstats'month-year'virtual-labs.ac.in.txt=. The page views
    from these files are only required. The function below looks
    through only the specified files to obtain the page views. It
    returns the total page views of labs deployed on IIIT
    infrastructure.
#+BEGIN_SRC python :tangle analytics/extract_data.py
def extract_iiit_data(year, location):
    files = get_file_names(year,location);
    dictionary = {}
    dict_tot_pages = {}
    for i in files:
        if re.search(r'awstats\d+.virtual-labs.ac.in.txt$',i):
            print i
            f = open(i)
            for line in f:
                if re.match(r'^BEGIN_DAY',line):
                    n = int((line.strip('BEGIN_DAY ')).rstrip())
                    x = 0
                    p = 0
                    for page_line in f:
                        if (x >= n):
                            break
                        p += int(page_line.split()[1])
                        x += 1
                    f.close()
                    dictionary[(i.lstrip('awstats.')).rstrip('.txt')] = p
                    break
    z = 0
    for key in dictionary:
        z += dictionary[key]
    return z
#+END_SRC
    The total page views from both the platforms is obtained from the
    function below. The statistics were collected from the year
    2013-2015 hence the years are passed as parameters to the above
    functions.
#+BEGIN_SRC python :tangle analytics/extract_data.py
def grand_total():
    global __dict_tot_pages__
    year = [ '2013','2014', '2015' ]
    aws_location = '/root/aws/'
    base_location = '/root/base/'
    tot_pages=0
    for i in year:
        tot_pages += extract_iiit_data(i,base_location) + extract_aws_data(i,aws_location)
    __dict_tot_pages__["total-pages"]=str(tot_pages)
    return __dict_tot_pages__["total-pages"]
#+END_SRC

*** COMMENT Total usage extraction 
    The purpose of this script is described in [[How%20the%20data%20is%20processed][How the data is
    processed]] for [[Requirement%20#2][requirement #2]]. The python script is as below.
    The python script is as described below. 
    Import the necessary packages, files and variables.
#+BEGIN_SRC python :tangle analytics/usage.py
import shlex
import os
import csv
import fnmatch
import re
import aws_config
#import deploy_config
import config

__current_path__ = '/var/www/html/analytics/'
#+END_SRC
    The apache log files need to be converted into comma separated
    format(csv files) for parsing the data easily.
#+BEGIN_SRC 
def logs_to_csv(path):
    global __current_path__
    files_dict = {}
    log_as_csv_files = []
    for file in os.listdir(path):
#        print file
        if fnmatch.fnmatch(file, '*access_log*'):
            lab_id = file.split('.')[0]
            if not files_dict.has_key(lab_id):
                files_dict[lab_id] = []
                files_dict[lab_id].append(file)
            else:
                files_dict[lab_id].append(file)
#    print files_dict 

    for key in files_dict:
        lab_log_files = files_dict[key]
        lab_csv_file = key+'.csv'
        log_as_csv_files.append(lab_csv_file)
        with open(current_path + lab_csv_file, 'w') as csv_file:
            for log in lab_log_files:
                with open(path+log) as input:
                    for line in input:
                        output = ','.join(shlex.split(line)) + '\n'
                        csv_file.write(output)
    return log_as_csv_files

def get_lab_list(lab_csv_file):
    lab_usage_count = 0
    lab_id = lab_csv_file.split('.')[0]
#    print "the lab id is: %s" %(lab_id)
    lab_exp_list = aws_config.url_sim.get(lab_id)
#    print "this is the labs experiment and simulation list : %s" %(lab_exp_list)
    lab_usage_count += match_exp(lab_exp_list, lab_csv_file)
#    print "the lab usage count is: %s" %(lab_usage_count)
    return lab_usage_count

def match_exp(exp_and_sim_list, lab_csv_file):
    global __current_path__
    exp_usage =0
    if exp_and_sim_list == None:
        return 0
    #print "lab csv file is: %s" %(lab_csv_file)
    csv_file= open(__current_path__ + lab_csv_file, 'rb')
    for item in exp_and_sim_list:
        lines_in_file = csv.reader(csv_file, delimiter=',', quotechar='\n')
#        print "item is %s" %(item)
        exp_url = item[0]
#        print "the exp_url is : %s" %(exp_url)
        sim_pattern = item[1]
        experiment_list = []
        for row in lines_in_file:
        #print "this is row %s" %(row)
            request = row[5]
            #print exp_url
            #print "this is exp_url %s \n" %(exp_url)
#            print "this is the request %s" %(request)
            if re.search(exp_url, request):
#                print "EEEEEEEEEEEEEEEEEEEEE"
                experiment_list.append(request)
        csv_file.seek(0)
#        print "this is the matched list of experiments :%s" %(experiment_list)
        exp_usage+=usage(experiment_list,sim_pattern)
#        print "this is the experiment usage: %d" %(exp_usage)
    return exp_usage

def usage(experiment_list,sim_pattern):
    req = False
    usage = 0
    for item in experiment_list:
        sim = re.search(sim_pattern,item)
#        print type(sim)
        if sim and req==True:
#            print "slnvoindbov"
            usage+=1
            req=False
        elif not sim:
            req=True
    return usage

#if __name__ == "__main__":
def tot_usage():
    dict_tot_usage = {}
    total_usage = 0
    csv_files_list=logs_to_csv(config.path)
    #print "the csv file list is : %s" %(csv_files_list)
    lab_usage = map(get_lab_list, csv_files_list)
    for i in lab_usage:
        total_usage += i
    dict_tot_usage["total-usage"]=str(total_usage)
    return dict_tot_usage["total-usage"]
    #print "the total usage is: %s" %(total_usage)

def usage_hits():
    return "1234"

print tot_usage()
#+END_SRC

    There are two different config files, one which enlists the labs
    on AWS and the other for deploy. The config file for labs on AWS
    is as below:
#+BEGIN_SRC python :tangle analytics/config/aws_config.py
url_sim = {
'emt-iiith':[['Experiment.php\?code\=C001','T005\&code\=C001'],
             ['Experiment.php\?code\=C002','T005\&code\=C002'],
             ['Experiment.php\?code\=C009','T005\&code\=C009']], 
'cse01-iiith':[['http://cse01-iiith.virtual-labs.ac.in/exp1/index.php','Simulation'],
               ['http://cse01-iiith.virtual-labs.ac.in/exp2/index.php','Simulation'],
               ['http://cse01-iiith.virtual-labs.ac.in/exp3/index.php','Simulation']]
}
#+END_SRC
    The config file for deploy is as below:
#+BEGIN_SRC python :tangle analytics/config/deploy_config.py
url_sim = {
'cse04':[['/labs/cse04/exp1/index.php','Experiment'],
         ['/labs/cse04/exp2/index.php','Experiment'],
         ['/labs/cse04/exp3/index.php','Experiment']], 
'cse07':[['/labs/cse07/exp1/index.php','Experiment'],
         ['/labs/cse07/exp2/index.php','Experiment'],
         ['/labs/cse07/exp3/index.php','Experiment']]
}
#+END_SRC
#+BEGIN_SRC python :tangle analytics/config/config.py
path='/var/www/html/logs/'
#+END_SRC
*** Usage Script
To convert logs to csv we use the following script:
#+BEGIN_SRC python :tangle logs_to_csv.py 
import os
import csv
import fnmatch
import re
import config

#__current_path__ = '/var/www/html/analytics/deploy-csv/'
def logs_to_csv(path_for_logs,path_for_csv_files):
#    global __current_path__
    files_dict = {}
    log_as_csv_files = []
    for file in os.listdir(path_for_logs):
#        print file
        if fnmatch.fnmatch(file, '*access_log*'):
            lab_id = file.split('.')[0]
            if not files_dict.has_key(lab_id):
                files_dict[lab_id] = []
                files_dict[lab_id].append(file)
            else:
                files_dict[lab_id].append(file)
    print files_dict

    for key in files_dict:
        lab_log_files = files_dict[key]
        lab_csv_file = key+'.csv'
        log_as_csv_files.append(lab_csv_file)
        with open(path_for_csv_files + lab_csv_file, 'w') as csv_file:
            for log in lab_log_files:
                with open(path_for_logs+log) as input:
                    for line in input:
                        output = ','.join(shlex.split(line)) + '\n'
                        csv_file.write(output)


if __name__ == "__main__":
    logs_to_csv(config.aws_path, config.aws_csv_path)
    logs_to_csv(config.deploy_path, config.deploy_csv_path)
#+END_SRC
To generate the usage count we use the following script: 
#+BEGIN_SRC python :tangle build_exp_list.py 
import os
import aws_config 
import deploy_config 
import config
 
sim_count = 0
ctx_count = 0
def filter(csv_path, filter_path, exp_sim_urls_for_labs):
    global sim_count 
    global ctx_count 
    usage = 0
#    files_dir = "/var/www/html/analytics/csv/"
    files_dir = csv_path
    for key in exp_sim_urls_for_labs.keys():
        file_name = "%s%s.csv" % (files_dir, key)
        pattern_list = exp_sim_urls_for_labs[key]
        file = open(file_name, 'r')
#        new_file_name = "/var/www/html/analytics/filter/filtered_%s" % key
        new_file_name = filter_path + "filtered_%s" % key
        new_file = open(new_file_name, 'w+')
        columns = []
        for line in file:
            columns = line.split(',')
            if (search_pattern(columns[5], pattern_list)):
                new_file.write(columns[5] + "\n")
        new_file.write("Simulation count = %d" % sim_count)
        new_file.write("Content count = %d" % (ctx_count - sim_count))
        
        if (ctx_count >= sim_count):
            usage += sim_count
        
        ctx_count = 0
        sim_count = 0

    return usage
#+END_SRC

This function will calculate the usage by taking the total count of
content and simulations and calculates the usage only if simulation
count is less than content count and takes the simulation count as
usage.

#+BEGIN_SRC python :tangle build_exp_list.py 
def search_pattern(get_field, pattern_list):
    ret_val = False
    global ctx_count
    global sim_count
    for experiment in pattern_list:
        if experiment[0] in get_field:
            ctx_count += 1
            ret_val = True
        if experiment[1] in get_field:
            sim_count += 1
            ret_val = True

    return ret_val


def total_usage():    
    aws_usage = filter(config.aws_csv_path, config.aws_filter_path, aws_config.exp_sim_urls_for_labs)
    deploy_usage = filter(config.deploy_csv_path, config.deploy_filter_path, deploy_config.exp_sim_urls_for_labs)
    tot_usage = aws_usage + deploy_usage 
    print "Usage = %s" % tot_usage
    return str(tot_usage)

if __name__ == "__main__":
    usage = total_usage()
    print "Usage = %s" % usage

#+END_SRC

The config file:
#+BEGIN_SRC python :tangle config.py
aws_path = '/var/www/html/aws-logs/'
deploy_path = '/var/www/html/deploy-logs/'

aws_csv_path = '/var/www/html/analytics/aws-csv/'
deploy_csv_path = '/var/www/html/analytics/deploy-csv/'

aws_filter_path = '/var/www/html/analytics/aws-filter/'
deploy_filter_path = '/var/www/html/analytics/deploy-filter/'
#+END_SRC
The AWS config file:
#+BEGIN_SRC python :tangle aws_config.py 
exp_sim_urls_for_labs = { 
'cse01-iiith':[['/exp1/index.php','/exp1/index.php?section=Simulation'],
        ['/exp2/index.php','/exp2/index.php?section=Simulation'],	
        ['/exp3/index.php','/exp3/index.php?section=Simulation'],
	['/exp4/index.php','/exp4/index.php?section=Simulation'],
	['/exp5/index.php','/exp5/index.php?section=Simulation'],
	['/exp6/index.php','/exp6/index.php?section=Simulation'],
	['/exp7/index.php','/exp7/index.php?section=Simulation'],
	['/exp8/index.php','/exp8/index.php?section=Simulation'],
	['/exp9/index.php','/exp9/index.php?section=Simulation']],
'cse02-iiith':[	['/exp1/index.php','/exp1/index.php?section=Experiment'],
	['/exp2/index.php','/exp2/index.php?section=Experiment'],
	['/exp3/index.php','/exp3/index.php?section=Experiment'],
	['/exp4/index.php','/exp4/index.php?section=Experiment'],
	['/exp5/index.php','/exp5/index.php?section=Experiment'],
	['/exp6/index.php','/exp6/index.php?section=Experiment'],
	['/exp7/index.php','/exp7/index.php?section=Experiment'],
	['/exp8/index.php','/exp8/index.php?section=Experiment'],
	['/exp9/index.php','/exp9/index.php?section=Experiment'],
	['/exp10/index.php','/exp10/index.php?section=Experiment']],
'cse15-iiith':[['/exp1/index.php','/exp1/index.php?section=Experiment'],
	['/exp2/index.php','/exp2/index.php?section=Experiment'],
	['/exp3/index.php','/exp3/index.php?section=Experiment'],
	['/exp4/index.php','/exp4/index.php?section=Experiment'],
	['/exp5/index.php','/exp5/index.php?section=Experiment'],
	['/exp6/index.php','/exp6/index.php?section=Experiment'],
	['/exp7/index.php','/exp7/index.php?section=Experiment'],
	['/exp8/index.php','/exp8/index.php?section=Experiment'],
	['/exp9/index.php','/exp9/index.php?section=Experiment'],
	['/exp10/index.php','/exp10/index.php?section=Experiment']],
'cse18-iiith':[['/exp1/index.php','/exp1/index.php?section=Experiment'],
	['/exp2/index.php','/exp2/index.php?section=Experiment'],
	['/exp3/index.php','/exp3/index.php?section=Experiment'],	
        ['/exp4/index.php','/exp4/index.php?section=Experiment'],
	['/exp5a/index.php','/exp5a/index.php?section=Experiment'],
	['/exp5b/index.php','/exp5b/index.php?section=Experiment'],
	['/exp6/index.php','/exp6/index.php?section=Experiment'],
	['/exp7/index.php','/exp7/index.php?section=Experiment'],
	['/exp8/index.php','/exp8/index.php?section=Experiment'],
	['/exp9/index.php','/exp9/index.php?section=Experiment'],
	['/exp10/index.php','/exp10/index.php?section=Experiment']],
'cse20-iiith':[['/exp1/index.php','/exp1/index.php?section=Experiment'],
	['/exp2/index.php','/exp2/index.php?section=Experiment'],
	['/exp3/index.php','/exp3/index.php?section=Experiment'],
	['/exp4/index.php','/exp4/index.php?section=Experiment'],
	['/exp5/index.php','/exp5/index.php?section=Experiment'],
	['/exp6/index.php','/exp6/index.php?section=Experiment'],
	['/exp7/index.php','/exp7/index.php?section=Experiment'],
	['/exp10/index.php','/exp10/index.php?section=Experiment']],
'emt-iiith':[['/Experiment.php?code=C001','/Experiment.php?T005&code=C001'],
	['/Experiment.php?code=C002','/Experiment.php?tid=T005&code=C002'],
	['/Experiment.php?code=C003','/Experiment.php?tid=T005&code=C003'],
	['/Experiment.php?code=C004','/Experiment.php?tid=T005&code=C004'],
	['/Experiment.php?code=C005','/Experiment.php?tid=T005&code=C005'],
	['/Experiment.php?code=C006','/Experiment.php?tid=T005&code=C006'],
	['/Experiment.php?code=C007','/Experiment.php?tid=T005&code=C007'],
	['/Experiment.php?code=C008','/Experiment.php?tid=T005&code=C008'],
	['/Experiment.php?code=C009','/Experiment.php?tid=T005&code=C009'],
	['/Experiment.php?code=C009','/Experiment.php?tid=T005&code=C009']],
'cse29-iiith':[['/exp1/index.php','/exp1/index.php?section=Experiment'],
	['/exp2/index.php','/exp2/index.php?section=Experiment'],
	['/exp3/index.php','/exp3/index.php?section=Experiment'],
	['/exp5/index.php','/exp5/index.php?section=Experiment'],
	['/exp6/index.php','/exp6/index.php?section=Experiment'],
	['/exp7/index.php','/exp7/index.php?section=Experiment'],
	['/exp8/index.php','/exp8/index.php?section=Experiment'],
	['/exp9/index.php','/exp9/index.php?section=Experiment'],
	['/exp10/index.php','/exp10/index.php?section=Experiment'],
	['/exp11/index.php','/exp11/index.php?section=Experiment']],
'ccnsb06-iiith':[['/exp6/index.php','/exp6/index.php?section=Experiment'],
	['/exp7/index.php','/exp7/index.php?section=Experiment'],
	['/exp8/index.php','/exp8/index.php?section=Experiment'],
	['/exp9/index.php','/exp9/index.php?section=Experiment'],
	['/exp5/index.php','/exp5/index.php?section=Experiment'],
	['/exp10/index.php','/exp10/index.php?section=Experiment'],
	['/exp1/index.php','/exp1/index.php?section=Experiment'],
	['/exp2/index.php','/exp2/index.php?section=Experiment']],
'eerc01-iiith':[['/exp1/index.php','/exp1/index.php?section=Experiment'],
	['/exp3/index.php','/exp3/index.php?section=Experiment'],
	['/exp4/index.php','/exp4/index.php?section=Experiment'],
	['/exp4/index.php','/exp4/index.php?section=Experiment'],
	['/exp5/index.php','/exp5/index.php?section=Experiment'],
	['/exp6/index.php','/exp6/index.php?section=Experiment'],
	['/exp7/index.php','/exp7/index.php?section=Experiment'],
	['/exp8/index.php','/exp8/index.php?section=Experiment'],
	['/exp9/index.php','/exp9/index.php?section=Experiment'],
	['/exp10/index.php','/exp10/index.php?section=Experiment']],
'csc-iiith':[['/exp1/index.html','/exp1/sol.html'],
	['/exp2/index.html','/exp2/gel_form.html'],
	['/exp3/index.html','/exp3/clean.html'],
	['/exp14/index.html','/exp14/cmc.html'],
	['/exp4/index.html','/exp4/surface.html'],
	['/exp5/index.html','/exp5/Tyndal.html'],
	['/exp7/index.html','/exp7/Gel_total.html'],
	['/exp8/index.html','/exp8/surface.html'],
	['/exp15/index.html','/exp15/lab9.html']],
'cse19-iiith':[['/objective.php?exp=diff','/diff.php'],
	['/objective.php?exp=arith','/arith.php'],
	['/objective.php?exp=affine','/affine.php'],
	['/objective.php?exp=point','/point.php'],
	['/objective.php?exp=neigh','/neigh.php'],
	['/objective.php?exp=histo','/histo.php'],
	['/objective.php?exp=fourier','/fourier.php'],
	['/objective.php?exp=colour','/colour.php'],
	['/objective.php?exp=morph','/morph.php'],
	['/objective.php?exp=segment','/segment.php'],
	['/objective.php?exp=piping','/piping.php']],
'cse22-iiith':[['/exp1/index.html','/exp1/index.html'],
	['/exp2/index.html','/exp2/index.html'],
	['/exp3/index.html','/exp3/index.html'],
	['/exp4/index.html','/exp4/index.html'],
	['/exp5/index.html','/exp5/index.html'],
	['/exp6/index.html','/exp6/index.html'],
	['/exp7/index.html','/exp7/index.html'],
	['/exp8/index.html','/exp8/index.html'],
	['/exp9/index.html','/exp9/index.html'],
	['/exp10/index.html','/exp10/index.html']],
'cse11-iiith':[['/Integers/index.php','section=Experiment'],
	['/FloatingPointNumbers/index.php','section=Experiment'],
	['/CacheOrganizations/index.php','section=Experiment'],
	['/LocalityAnalysis/index.php','virtual-labs.ac.in/loc'],
	['/TilingMatrix/index.php','virtual-labs.ac.in/matpar'],
	['/VirtualMemory/index.php','section=Experiment'],
	['/MIPS1/index.php','/MIPS1/index.php?section=Experiment'],
	['/MIPS2/index.php','/MIPS2/index.php?section=Experiment'],
	['/ARM1/index.php','/ARM1/index.php?section=Experiment'],
	['/ARM2/index.php','/ARM2/index.php?section=Experiment'],
	['/SingleCycle/index.php','section=Experiment']],
'cse14-iiith':[['/Experiment.php?code=C001','tid=T005&code=C001'],
	['/Experiment.php?code=C002','tid=T005&code=C002'],
	['/Experiment.php?code=C003','tid=T005&code=C003'],
	['/Experiment.php?code=C004','tid=T005&code=C004'],
	['/Experiment.php?code=C005','tid=T005&code=C005'],
	['/Experiment.php?code=C006','tid=T005&code=C006'],
	['/Experiment.php?code=C008','tid=T005&code=C008'],
	['/Experiment.php?code=C009','tid=T005&code=C009'],
	['/Experiment.php?code=C0091','tid=T005&code=C0091'],
	['/Experiment.php?code=C007','tid=T005&code=C007']],
'vem-iitg':[['/Magnetic_Field_Behaviour_in_single_coil.html','/Magnetic_Field_Behaviour_in_single_coil%28experiment1%29.html'],
	['/Magnetic_Field_Behaviour_in_single_coil.html','/Magnetic_Field_Behaviour_in_single_coil%28experiment2%29.html'],
	['/Rotating%20Magnetic%20Field%20Behaviour%20in%20two%20coils.html','/Rotating%20Magnetic%20Field%20Behaviour%20in%20two%20coils%28experiments%29.html'],
	['/Rotating%20Magnetic%20Field%20Behaviour%20in%20three%20coils.html','/Rotating_Magnetic_Field_Behaviour_in_three_coils%28experiments%29.html'],
	['/Introduction_DC.html','/Experiment_DC.html'],
	['/Introduction_Noload.html','/Experiment_Noload.html'],
	['/Introduction_blockr.html','/Experiment_blockr.html'],
	['/Stator%20Resistance%20Starter%28intro%29.html','/Stator%20Resistance%20Starter%28experiment%29.html'],
	['/Auto%20Transformer%20Starting%28intro%29.html','/Auto%20Transformer%20Starting%28experiment%29.html'],
	['/Star%20Delta%20Starting%28intro%29.html','/Star%20Delta%20Starting%28experiment%29.html']],
'bmi-iitr':[['/exp1/index.php','/exp1/index.php?section=Experiment'],
	['/exp2/index.php','/exp2/index.php?section=Experiment'],
	['/exp3/index.php','/exp3/index.php?section=Experiment'],
	['/exp5/index.php','/exp5/index.php?section=Experiment'],
	['/exp6/index.php','/exp6/index.php?section=Experiment'],
	['/exp7/index.php','/exp7/index.php?section=Experiment'],
	['/exp8/index.php','/exp8/index.php?section=Experiment']],
'bmsip-iitr':[['/exp1/index.php','/exp1/index.php?section=Experiment'],
	['/exp2/index.php','/exp2/index.php?section=Experiment'],
	['/exp3/index.php','/exp3/index.php?section=Experiment']],
'em-iitr':[['/exp1/index.php','/exp1/index.php?section=Experiment'],
	['/exp2/index.php','/exp2/index.php?section=Experiment'],
	['/exp3/index.php','/exp3/index.php?section=Experiment'],
	['/exp4/index.php','/exp4/index.php?section=Experiment'],
	['/exp5/index.php','/exp5/index.php?section=Experiment'],
	['/exp6/index.php','/exp6/index.php?section=Experiment'],
	['/exp7/index.php','/exp7/index.php?section=Experiment']],
'sl-iitr':[['/exp1/index.php','/exp1/index.php?section=Experiment'],
	['/exp2/index.php','/exp2/index.php?section=Experiment'],
	['/exp3/index.php','/exp3/index.php?section=Experiment'],
	['/exp5/index.php','/exp5/index.php?section=Experiment'],
	['/exp6/index.php','/exp6/index.php?section=Experiment'],
	['/exp7/index.php','/exp7/index.php?section=Experiment'],
	['/exp8/index.php','/exp8/index.php?section=Experiment'],
	['/exp9/index.php','/exp9/index.php?section=Experiment'],
	['/exp10/index.php','/exp10/index.php?section=Experiment'],
	['/exp11/index.php','/exp11/index.php?section=Experiment']],
'pev-au':[['/Interspecific_Competition_and_Coexistence/','/index.php?exp_id=Interspecific_Competition_and_Coexistence'],
	['/conservingEndangeredSpecies/','/index.php?exp_id=conservingEndangeredSpecies'],
	['/Interspecific%20Competition%20and%20Geographic%20Distributions/','/index.php?exp_id=Interspecific%20Competition%20and%20Geographic%20Distributions'],
	['/Metapopulation_Dynamics/','/index.php?exp_id=Metapopulation_Dynamics'],
	['/Parasitoid_Host_Dynamics/','/index.php?exp_id=Parasitoid_Host_Dynamics'],
	['/Spread_Pest_Population_invasion/','/index.php?exp_id=Spread_Pest_Population_invasion']],
'pevii-au':[['/OptimalForAging/','/index.php?exp_id=OptimalForAging'],
	['/OptimalForaging_Pollinators/','/index.php?exp_id=OptimalForaging_Pollinators'],
	['/Optimal_foraging_Sit_and_wait_predators_that_maximize_energy/','/index.php?exp_id=Optimal_foraging_Sit_and_wait_predators_that_maximize_energy']],
'anthropology-iitg':[['/Theory_handaxe.html','/Simulator_handaxe.html'],
	['/Theory_finger.html','/Simulator_finger.html'],
	['/Theory_pot.html','/Simulator_pot.html'],
	['/Theory_pot.html','/Simulator_pot.html'],
	['/Theory_pot.html','/Simulator_pot.html'],
	['/Theory_pot.html','/Simulator_pot.html'],
	['/Theory_indices.html','/Simulator_indices.html'],
	['/obtain_main_line_formula_theory.html','/obtain_main_line_formula_simulator.html'],
	['/Skeleton_Assembling_Identification_labeling_theory.html','/Skeleton_Assembling_Identification_labelin_simulator.html'],
	['/blood_group_technique_theory.html','/blood_group_technique_simulator.html'],
	['/hunting_fishing_tools_theory.html','/hunting_fishing_tools_simulator.html'],
	['/hunting_fishing_tools_theory.html','/hunting_fishing_tools_simulator.html']],
'ergonomics-iitg':	[['/Theory_static.html','/Practical%20Experience_static.html'],
	['/Theory_arm.html','/Practical%20Experience_arm.html'],
	['/Theory_sitting.html','/Practical%20Experience_sitting.html'],
	['/Theory_4.html','/Practical%20Experience_4.html'],
	['/Theory_5.html','/Practical%20Experience_5.html'],
	['/Theory_6.html','/Practical%20Experience_6.html'],
	['/Theory_7.html','/Practical%20Experience_7.html'],
	['/Theory_8.html','/Practical%20Experience_8.html'],
	['/Theory_9.html','/Practical%20Experience_9.html'],
	['/Theory_10.html','/Practical%20Experience_10.html']],
'plc-coep':[['/exp2/index.html','/exp2/simulator2.html'],
	['/exp3/index.html','/exp3/simulator3.html'],
	['/exp4/index.html','/exp4/simulator4.html'],
	['/exp5/index.html','/exp5/simulator5.html'],
	['/exp6/index.html','/exp6/simulator6.html'],
	['/exp7/index.html','/exp7/simulator7.html'],
	['/exp8/index.html','/exp8/simulator8.html']],
'em-coep':[['/Exp1/index.html','/Exp1/Experiment1.html'],
	['/Exp2/index.html','/Exp2/Experiment2.html'],
	['/Exp3/index.html','/Exp3/Experiment3.html'],
	['/Exp4/index.html','/Exp4/Experiment4.html'],
	['/Exp5/index.html','/Exp5/Experiment5.html'],
	['/Exp6/index.html','/Exp6/Experiment6.html'],
	['/Exp7/index.html','/Exp7/Experiment7.html'],
	['/Exp8/index.html','/Exp8/Experiment8.html']],
'sl-coep':[['/Rtd/index.html','/Rtd/rtd.html'],
	['/Biosensor/index.html','/Biosensor/biosensor.html'],
	['/Capacitance/index.html','/Capacitance/capacitance.html'],
	['/LinearVariableDifferntialTransformer/index.html','/LinearVariableDifferntialTransformer/lvdt.html'],
	['/Orifice/index.html','/Orifice/orifice.html'],
	['/PHCalculation/index.html','/PHCalculation/phc.html'],
	['/StrainGuage/index.html','/StrainGuage/strainguage.html'],
	['/Thermocouple/index.html','/Thermocouple/thermocouple.html']],
'he-coep':[['/Experiment1/index.html','/Experiment1/index1.html'],
	['/Experiment2/index.html','/Experiment2/index1.html'],
	['/Experiment3/index.html','/Experiment3/index1.html'],
	['/Experiment4/index.html','/Experiment4/index1.html'],
	['/Experiment5/index.html','/Experiment5/index1.html'],
	['/Experiment6/index.html','/Experiment6/index1.html'],
	['/Experiment8/index.html','/Experiment8/index1.html']],
'ial-coep':[['/Expt2/index.html','/Expt2/AnalogDigital.html'],
	['/Expt3/index.html','/Expt3/TimersCounters.html'],
	['/Expt4/index.html','/Expt4/TrafficControl.html'],
	['/Expt5/index.html','/Expt5/BottleFilling.html'],
	['/Expt6/index.html','/Expt6/HeatExchange.html'],
	['/Expt7/index.html','/Expt7/FBD.html']],
'va-coep':[['/MIofConnectingRod/index.html','/MIofConnectingRod/MIofConnectingRod.html'],
	['/ImpactTestCantilever/index.html','/ImpactTestCantilever/ImpactTestCantilever.html'],
	['/SineSweepCantilever/index.html','/SineSweepCantilever/SineSweepCantilever.html'],
	['/HarmonicExcitationSDOF/index.html','/HarmonicExcitationSDOF/HarmonicExcitationSDOF.html'],
	['/NaturalVibrationSDOF/index.html','/NaturalVibrationSDOF/NaturalVibrationSDOF.html'],
	['/TrifiliarSuspension/index.html','/TrifiliarSuspension/TrifiliarSuspension.html'],
	['/ArbitraryExcitation/index.html','/ArbitraryExcitation/ArbitraryExcitation.html'],
	['/TunedVibrationAbsorber/index.html','/TunedVibrationAbsorber/TunedVibrationAbsorber.html']],
'mm-coep':[['/PulsedHeating/index.html','/PulsedHeating/PulsedHeating.html'],
	['/LazerankosErosionModel/index.html','/LazerankosErosionModel/LazerankosErosionModel.html'],
	['/DibitontoVsSalonitis/index.html','/DibitontoVsSalonitis/DibitontoVsSalonitis.html'],
	['/EDM/index.html','/EDM/EDM.html'],
	['/YAGLaser/index.html','/YAGLaser/YAGLaser.html'],
	['/LaserSpotWelding/index.html','/LaserSpotWelding/LaserSpotWelding.html'],
	['/ECM/index.html','/ECM/ECM.html'],
	['/ECG/index.html','/ECG/ECG.html']],
'fab-coep':[['/exp1/index.html','/exp1/woodcutting.html'],
	['/exp2/index.html','/exp2/roughing.html'],
	['/exp3/index.html','/exp3/pcb.html'],
	['/exp4/index.html','/exp4/interfaceprog.html'],
	['/exp5/index.html','/exp5/flexibleCircuit.htm'],
	['/exp6/index.html','/exp6/scanning.html']],
'bmsp-coep':[['/AbnormalECG/index.html','/NormalECG/index.html'],
	['/AbnormalECG/index.html','/AbnormalECG/AbnormalECG.html'],
	['/Electroencephalogram/index.html','/Electroencephalogram/Electroencephalogram.html'],
	['/ElectromyogramSignal/index.html','/ElectromyogramSignal/ElectromyogramSignal.html'],
	['/Defibrillator/index.html','/Defibrillator/Defibrillator.html'],
	['/PaceMaker/index.html','/PaceMaker/PaceMaker.html'],
	['/PaceMaker/index.html','/SynchronousPaceMaker/index.html'],
	['/HaemoDialysis/index.html','/HaemoDialysis/HaemoDialysis.html'],
	['/EMGAmplifier/index.html','/EMGAmplifier/EMGAmplifier.html'],
	['/PulseMissingDetector/index.html','/PulseMissingDetector/PulseMissingDetector.html'],
	['/LeadECG/index.html','/LeadECG/LeadECG.html']],
'sa-nitk':[['/exp1/index.html','/exp1/index.html'],
	['/exp2/index.html','/exp2/index.html'],
	['/exp3/index.html','/exp3/index.html'],
	['/exp4/index.html','/exp4/index.html'],
	['/exp5/index.html','/exp5/index.html'],
	['/exp6/index.html','/exp6/index.html'],
	['/exp7/index.html','/exp7/index.html'],
	['/exp8/index.html','/exp8/index.html'],
	['/exp9/index.html','/exp9/index.html'],
	['/exp10/index.html','/exp10/index.html'],
	['/exp11/index.html','/exp11/index.html']],
'fm-nitk':[['/exp1/index.html','/exp1/index.html']],
'sm-nitk':[['/exp1/index.html','/exp1/index.html'],
	['/exp2/index.html','/exp2/index.html'],
	['/exp3/index.html','/exp3/index.html'],
	['/exp4/index.html','/exp4/index.html']],
'mm-nitk':[['/exp1/index.html','/exp1/index.html'],
	['/exp2/index.html','/exp2/index.html'],
	['/exp3/index.html','/exp3/index.html'],
	['/exp4/index.html','/exp4/index.html'],
	['/exp5/index.html','/exp5/index.html'],
	['/exp6/index.html','/exp6/index.html'],
	['/exp7/index.html','/exp7/index.html'],
	['/exp8/index.html','/exp8/index.html'],
	['/exp9/index.html','/exp9/index.html'],
	['/exp10/index.html','/exp10/index.html'],
	['/exp11/index.html','/exp11/index.html'],
	['/exp12/index.html','/exp12/index.html'],
	['/exp13/index.html','/exp13/index.html'],
	['/exp14/index.html','/exp14/index.html'],
	['/exp15/index.html','/exp15/index.html'],
	['/exp16/index.html','/exp16/index.html']],
'mdmv-nitk':[['/exp1/index.html','/exp1/index.html'],
	['/exp2/index.html','/exp2/index.html'],
	['/exp3/index.html','/exp3/index.html'],
	['/exp4/index.html','/exp4/index.html'],
	['/exp5/index.html','/exp5/index.html'],
	['/exp6/index.html','/exp6/index.html'],
	['/exp7/index.html','/exp7/index.html'],
	['/exp8/index.html','/exp8/index.html']],
'uorepc-nitk':[['/exp1/index.html','/exp1/index.html'],
	['/exp2/index.html','/exp2/index.html'],
	['/exp3/index.html','/exp3/index.html'],
	['/exp4/index.html','/exp4/index.html'],
	['/exp5/index.html','/exp5/index.html'],
	['/exp6/index.html','/exp6/index.html'],
	['/exp7/index.html','/exp7/index.html'],
	['/exp8/index.html','/exp8/index.html'],
	['/exp9/index.html','/exp9/index.html']],
'ied-nitk':[['/INTRODUCTION%20TO%20PROGRAMMABLE%20LOGIC%20CONTROLLER%20AND%20INTRODUCTION%20TO%20DIGITAL%20IO%20INTERFACE%20TO%20PLC/index.html','/INTRODUCTION%20TO%20PROGRAMMABLE%20LOGIC%20CONTROLLER%20AND%20INTRODUCTION%20TO%20DIGITAL%20IO%20INTERFACE%20TO%20PLC/index.html'],
	['/INTRODUCTION%20TO%20LADDER%20LOGIC/index.html','/INTRODUCTION%20TO%20LADDER%20LOGIC/index.html'],
	['/ptinst/index.html','/PLC%20On-Delay%20Timer%20Instruction/index.html'],
	['/ptinst/index.html','/PLC%20Off-Delay%20Timer%20Instruction/index.html'],
	['/ptinst/index.html','/PLC%20Retentive%20Timer%20On%20%20Instruction/index.html'],
	['/plcinst/index.html','/PLC%20COUNT-UP%20INSTRUCTION/index.html'],
	['/plcinst/index.html','/PLC%20COUNT-DOWN%20INSTRUCTION/index.html'],
	['/appplc/index.html','/Garage%20Shutter%20Opening%20and%20Closing%20Using%20PLC/index.html'],
	['/appplc/index.html','/Container%20Filling%20Process%20Using%20PLC/index.html'],
	['/appplc/index.html','/Simultaneous%20output%20interlock%20using%20PLC/index.html'],
	['/appplc/index.html','/Maximum%20Simultaneous%20Operations%20Limiter%20using%20PLC/index.html'],
	['/appplc/index.html','/Motor%20forward%20and%20reverse%20direction%20control%20using%20PLC/index.html']],
'qnm-iitd':[['/experiment1.html','/Experiment%201/mm1.html'],
	['/experiment1.html','/Experiment%201/mmc.html'],
	['/experiment1.html','/Experiment%201/mminf.html'],
	['/experiment2.html','/Experiment%202/Layout.html'],
	['/experiment3.html','/Experiment%203/Layout.html'],
	['/experiment4.html','/Experiment%204/Exp4a.html'],
	['/experiment4.html','/Experiment%204/Exp4b.html'],
	['/experiment5.html','/Experiment%205/Exp5a.html'],
	['/experiment5.html','/Experiment%205/Exp5b.html'],
	['/experiment6.html','/Experiment%206/Exp6.html'],
	['/experiment7.html','/Experiment%207/DT1a/build/mm1.html'],
	['/experiment7.html','/Experiment%207/DT1b/build/mmc.html'],
	['/experiment7.html','/Experiment%207/DT1c/build/mminf.html'],
	['/experiment7.html','/Experiment%207/DT2/build/Layout.html'],
	['/experiment7.html','/Experiment%207/DT3/build/Layout.html'],
	['/experiment8.html','/Experiment%208/tandem.html'],
	['/experiment9.html','/Experiment%209/model2.html'],
	['/experiment10.html','/Experiment%2010/opennetwork.html'],
	['/experiment11.html','/Experiment%2011/closednetwork.html'],
	['/experiment12.html','/Experiment%2012/12a.html'],
	['/experiment12.html','/Experiment%2012/12b.html']],
'eem-iitd':[['/exp1.html','/exp1/Application.exe'],
	['/exp2.html','/exp2/Application.exe'],
	['/exp3.html','/exp3/Application.exe'],
	['/exp4.html','/exp4/Application.exe'],
	['/exp5.html','/exp5/Application.exe'],
	['/exp7.html','/exp7/Application.exe'],
	['/exp8.html','/exp8/Application.exe']],
'msvs-dei':[['/upsetting_process.php','/upsetting_simulation.php'],
	['/upsetting_process.php','/Hydraulic.php'],
	['/upsetting_process.php','/Mechanical.php'],
	['/upsetting_process.php','/Upset_Experiment.php?UPSETTING/Upset_Comp/Case_1.mp4'],
	['/upsetting_process.php','/Upset_Experiment.php?UPSETTING/Upset_Comp/Case_2.mp4'],
	['/upsetting_process.php','/Upset_Experiment.php?UPSETTING/Upset_Comp/Case_3.mp4'],
	['/upsetting_process.php','/Upset_Experiment.php?UPSETTING/Upset_Comp/Case_4.mp4'],
	['/Extrusion.php','/Aluminium.php'],
	['/Extrusion.php','/Titanium.php'],
	['/Extrusion.php','/Extru_Experiment.php?EXTRUSION/ExtruComparision/SolidPipe.mp4'],
	['/Extrusion.php','/Extru_Experiment.php?EXTRUSION/ExtruComparision/RadiusExtrusion.mp4'],
	['/Extrusion.php','/Extru_Experiment.php?EXTRUSION/ExtruComparision/ExtrusionAngles.mp4'],
	['/MultiStep.php','/MultiStep_Experiment.php?UPSETTING/MultiStep/MSF1.mp4'],
	['/MultiStep.php','/MultiStep_Experiment.php?UPSETTING/MultiStep/MSF2.mp4'],
	['/MultiStep.php','/MultiStep_Experiment.php?UPSETTING/MultiStep/MSF3.mp4'],
	['/MultiStep.php','/MultiStep_Experiment.php?UPSETTING/MultiStep/MSF4.mp4'],
	['/MultiStep.php','/MultiStep_Experiment.php?UPSETTING/MultiStep/MSF5.mp4'],
	['/Hammer_Forging.php','/Hammer_Experiment.php?UPSETTING/Hammer_Forging/HammerForging1.mp4'],
	['/Hammer_Forging.php','/Hammer_Experiment.php?UPSETTING/Hammer_Forging/HammerForging2.mp4'],
	['/Rolling_process.php','/Rolling.php'],
	['/Rolling_process.php','/RingRoll.php?ROLLING/RingRolling/FlatringSetup.mp4'],
	['/Rolling_process.php','/RingRoll.php?ROLLING/RingRolling/FlatringStrain.mp4'],
	['/Rolling_process.php','/RingRoll.php?ROLLING/RingRolling/CurvedringSetup.mp4'],
	['/Rolling_process.php','/RingRoll.php?ROLLING/RingRolling/CurvedringStrain.mp4'],
	['/ThreadRolling.php','/ThreadSimulation.php?ROLLING/ThreadRolling/FlatDies_Graph.mp4'],
	['/ThreadRolling.php','/ThreadSimulation.php?ROLLING/ThreadRolling/FlatDies_Strain.mp4'],
	['/ThreadRolling.php','/ThreadSimulation.php?ROLLING/ThreadRolling/TwoRolls_Graph.mp4'],
	['/ThreadRolling.php','/ThreadSimulation.php?ROLLING/ThreadRolling/TwoRolls_Graph.mp4'],
	['/ThreadRolling.php','/ThreadSimulation.php?ROLLING/ThreadRolling/ThreeRolls_Graph.mp4'],
	['/ThreadRolling.php','/ThreadSimulation.php?ROLLING/ThreadRolling/ThreeRolls_Strain.mp4'],
	['/WedgeRolling.php','/WedgeSimulation.php?Rolling/WedgeRolling/PlanetaryRolling.mp4'],
	['/WedgeRolling.php','/WedgeSimulation.php?Rolling/WedgeRolling/ThreeRolls.mp4'],
	['/SheetMetal.php','/SheetMetal_Experiment.php?SHEETMETAL/Bending/Bending1.mp4'],
	['/SheetMetal.php','/SheetMetal_Experiment.php?SHEETMETAL/Bending/Bending2.mp4'],
	['/SheetMetal.php','/SheetMetal_Experiment.php?SHEETMETAL/Bending/Bending4.mp4'],
	['/SheetMetal.php','/SheetMetal_Experiment.php?SHEETMETAL/Bending/Bending3.mp4'],
	['/SheetMetal.php','/SheetMetal_Experiment.php?SHEETMETAL/Bending/Bending5.mp4'],
	['/SheetMetal.php','/SheetMetal_Experiment.php?SHEETMETAL/Bending/Bending8.mp4'],
	['/SheetMetal.php','/SheetMetal_Experiment.php?SHEETMETAL/Bending/Bending6.mp4'],
	['/SheetMetal.php','/SheetMetal_Experiment.php?SHEETMETAL/Bending/Bending7.mp4'],
	['/Orbital.php','/OrbitalSimulation.php?Forming/Orbital/OrbitalForming.mp4'],
	['/Orbital.php','/OrbitalSimulation.php?Forming/Orbital/OrbitalStrain.mp4'],
	['/Orbital.php','/OrbitalSimulation.php?Forming/Orbital/FormingOrbital.mp4'],
	['/Orbital.php','/OrbitalSimulation.php?Forming/Orbital/OrbitalRiveting.mp4'],
	['/Hydroforming.php','/HydroSimulation.php?Forming/Hydroforming/TubeHydroforming.mp4'],
	['/Hydroforming.php','/HydroSimulation.php?Forming/Hydroforming/CurveTubeHydroforming.mp4'],
	['/Hydroforming.php','/HydroSimulation.php?Forming/Hydroforming/Hydroforming_ST.mp4'],
	['/Hydroforming.php','/HydroSimulation.php?Forming/Hydroforming/CurveHydroforming_ST.mp4'],
	['/Hydroforming.php','/HydroSimulation.php?Forming/Hydroforming/Hydroforming_dt_ss.mp4'],
	['/Hydroforming.php','/HydroSimulation.php?Forming/Hydroforming/Hydroforming_DT_os.mp4'],
	['/Hydroforming.php','/HydroSimulation.php?Forming/Hydroforming/SheetHydroforming.mp4'],
	['/Hydroforming.php','/HydroSimulation.php?Forming/Hydroforming/SheetStrainHydroforming.mp4'],
	['/Stretchforming.php','/StretchSimulation.php?Forming/Stretchforming/FormingStretch.mp4'],
	['/Stretchforming.php','/StretchSimulation.php?Forming/Stretchforming/StretchGraph.mp4'],
	['/Stretchforming.php','/StretchSimulation.php?Forming/Stretchforming/FormingStrain.mp4'],
	['/Stretchforming.php','/StretchSimulation.php?Forming/Stretchforming/FormingGraph.mp4'],
	['/Cogging.php','/CoggingSimulation.php?Cogging/CoggingCurve.mp4'],
	['/Cogging.php','/CoggingSimulation.php?Cogging/CoggingStrain.mp4'],
	['/Cogging.php','/CoggingSimulation.php?Cogging/BeckingSetup.mp4'],
	['/Cogging.php','/CoggingSimulation.php?Cogging/BeckingStrain.mp4'],
	['/Cogging.php','/CoggingSimulation.php?Cogging/MDShaftSetup.mp4'],
	['/Cogging.php','/CoggingSimulation.php?Cogging/MDShaftStrain.mp4'],
	['/Cogging.php','/CoggingSimulation.php?Cogging/MDShaftCurve.mp4'],
	['/Swaging.php','/SwagingSimulation.php?Swaging/Swaging_Kinematics.mp4'],
	['/Swaging.php','/SwagingSimulation.php?Swaging/Swaging_Strain.mp4'],
	['/Swaging.php','/SwagingSimulation.php?Swaging/SwagingGraph.mp4'],
        ['/Riveting.php','/RivetingSimulation.php?Riveting/RivetForming.mp4'],
        ['/Riveting.php','/RivetingSimulation.php?Riveting/RivetStrainForming.mp4'],
        ['/Riveting.php','/RivetingSimulation.php?Riveting/RivetCurveForming.mp4'],
        ['/Riveting.php','/RivetingSimulation.php?Riveting/RivetingSplit.mp4'],
        ['/Riveting.php','/RivetingSimulation.php?Riveting/SplitRiveting_Kinematics.mp4'],
        ['/Riveting.php','/RivetingSimulation.php?Riveting/SplitRiveting_CutSection.mp4'],
        ['/Riveting.php','/RivetingSimulation.php?Riveting/FlatRiveting.mp4'],
        ['/Riveting.php','/RivetingSimulation.php?Riveting/FlatStrainRiveting.mp4'],
        ['/Riveting.php','/RivetingSimulation.php?Riveting/FlatCurveRiveting.mp4'],
        ['/Quenching.php','/QuenchingSimulation.php?Quenching/QuenchingProcess.mp4'],
        ['/Quenching.php','/QuenchingSimulation.php?Quenching/StrainQuenching.mp4'],
        ['/rct.php','/RCT_Bench.php'],
        ['/Defects.php','/DefectSimulation.php?Defects/BucklingGraph.mp4'],
        ['/Defects.php','/DefectSimulation.php?Defects/Buckling_Strain.mp4'],
        ['/Defects.php','/DefectSimulation.php?Defects/Shearing_Kinematics.mp4'],
        ['/Defects.php','/DefectSimulation.php?Defects/Shearing_Strain.mp4'],
        ['/Defects.php','/DefectSimulation.php?Defects/OverFills.mp4'],
        ['/Defects.php','/DefectSimulation.php?Defects/UnderFills1.mp4'],
        ['/Defects.php','/DefectSimulation.php?Defects/UnderFills2.mp4'],
        ['/Defects.php','/DefectSimulation.php?Defects/MultipleDefects.mp4']],
'vp-dei':[['/Dreamweaver/exp1.html','/Dreamweaver/Sim_dcmotor.html'],
	['/Dreamweaver/exp1.html','/Dreamweaver/syn_Sim.html'],
	['/Dreamweaver/exp1.html','/Dreamweaver/Sim_DG_Set.html'],
	['/Dreamweaver/exp1.html','/Dreamweaver/dynDcmotor.html'],
	['/Dreamweaver/exp2.html','/Dreamweaver/Sim_2/phaseSequence.html'],
	['/Dreamweaver/exp2.html','/Dreamweaver/Sim_2/SlipTest/index.html'],
	['/Dreamweaver/exp3.html','/Dreamweaver/sim_3/OCC.html'],
	['/Dreamweaver/exp3.html','/Dreamweaver/sim_3/SCC.html'],
	['/Dreamweaver/exp3.html','/Dreamweaver/sim_3/comb/X2.html'],
	['/Dreamweaver/exp3.html','/Dreamweaver/sim_3/comb/X0.html'],
	['/Dreamweaver/exp4.html','/Dreamweaver/Sim_4/web/Exp4.html'],
	['/Dreamweaver/exp5.html','/Dreamweaver/sim_5/web/Exp5.html'],
	['/Dreamweaver/exp6.html','/Dreamweaver/sim_6/web/exp6.html'],
	['/Dreamweaver/exp6.html','/Dreamweaver/sim_6a/web/index.html'],
	['/Dreamweaver/exp7.html','/Dreamweaver/Sim%207/web/sim7.html'],
	['/Dreamweaver/exp8.html','/Dreamweaver/Sim8/web/Sim8.html'],
	['/Dreamweaver/exp9.html','/Dreamweaver/Sim9/web/index.html']],
'vc-dei':[['/?page_id=83','/?page_id=108'],
	['/?page_id=186','/?page_id=195'],
	['/?page_id=200','/?page_id=209'],
	['/?page_id=612','/?page_id=885'],
	['/?page_id=748','/?page_id=899'],
	['/?page_id=786','/?page_id=911'],
	['/?page_id=939','/?page_id=968'],
	['/?page_id=1028','/?page_id=1094'],
	['/?page_id=1111','/?page_id=1169']],
'pe-iitb':[['/exp3/index.html','/exp3/index.html'],
	['/exp4/index.html','/exp4/index.html'],
	['/exp5/index.html','/exp5/index.html'],
	['/exp7/index.html','/exp7/index.html'],
	['/exp8/index.html','/exp8/index.html'],
	['/exp9/index.html','/exp9/index.html'],
	['/exp10/index.html','/exp10/index.html'],
	['/exp11/index.html','/exp11/index.html'],
	['/exp13/index.html','/exp13/index.html'],
	['/exp14/index.html','/exp14/index.html'],
	['/exp15/index.html','/exp15/index.html'],
	['/exp16/index.html','/exp16/index.html']],
'ce-iitb':[['/exp1/index.html','/exp1/ori/ori1.php'],
	['/exp2/index.html','/exp2/gla/gla1.php'],
	['/exp3/index.html','/exp3/2pf/2pf1.php'],
	['/exp4/index.html','/exp4/hpc/hpc1.php'],
	['/exp5/index.html','/exp5/nof/nof1.php'],
	['/exp6/index.html','/exp6/calo/cal1.php'],
	['/exp7/index.html','/exp7/ftf/fm1.php'],
	['/exp8/index.html','/exp8/htl/htl.php'],
	['/exp8/index.html','/exp9/vle/vle.php'],
	['/exp10/index.html','coepvlab.ac.in:8080/COEP/Vlabs/Simulators/ElectricalLab/SensorLab/Rtd/index.html']],
'ee-iitb':[['/exp1/index.html','/exp1/index.html'],
	['/exp12/index.html','/exp12/index.html'],
	['/exp2/index.html','/exp2/index.html'],
	['/exp3/index.html','/exp3/index.html'],
	['/exp4/index.html','/exp4/index.html'],
	['/exp5/index.html','/exp5/index.html'],
	['/exp6/index.html','/exp6/index.html'],
	['/exp7/index.html','/exp7/index.html'],
	['/exp9/index.html','/exp9/index.html'],
	['/exp10/index.html','/exp10/index.html'],
	['/exp11/index.html','/exp11/index.html']],
'cds-iiith':[['/exp1/index.html','/exp1/swf.html'],
        ['/exp2/index.html','/exp2/emanim_ord_refrac.html'],
        ['/exp2/index.html','/exp2/part2_cd/emanim_cd_refrac.html'],
        ['/exp3/index.html','/exp3/jmol/'],
        ['/exp4/index.html','/exp4/exp/'],
        ['/exp5/index.html','/exp5/swf.html'],
        ['/exp6/index.html','/exp6/swf.html'],
        ['/exp7/index.html','/exp7/swf.html'],
        ['/exp8/index.html','/exp8/experiment/index.html'],
        ['/exp9/index.html','/exp9/experiment/index.html'],
        ['/exp10/index.html','/exp10/experiment/index.html']],
'smfe-iiith':[['/exp1/index.php','/exp1/index.php?section=Experiment'],
	['/exp2/index.php','/exp2/index.php?section=Experiment'],
	['/exp3/index.php','/exp3/index.php?section=Experiment'],
	['/exp4/index.php','/exp4/index.php?section=Experiment'],
	['/exp5/index.php','/exp5/index.php?section=Experiment'],
	['/exp6/index.php','/exp6/index.php?section=Experiment'],
	['/exp7/index.php','/exp7/index.php?section=Experiment'],
	['/exp8/index.php','/exp8/index.php?section=Experiment'],
	['/exp9/index.php','/exp9/index.php?section=Experiment'],
	['/exp10/index.php','/exp10/index.php?section=Experiment']],
'ssp-iiith':[['/exp01/index.html','/exp01/index.html'],
	['/exp02/index.html','/exp02/index.html'],
	['/exp03/index.html','/exp03/index.html'],
	['/exp04/index.html','/exp04/index.html'],
	['/exp05/index.html','/exp05/index.html'],
	['/exp06/index.html','/exp06/index.html'],
	['/exp07/index.html','/exp07/index.html'],
	['/exp08/index.html','/exp08/index.html'],
	['/exp09/index.html','/exp09/index.html'],
	['/exp10/index.html','/exp10/index.html']],
'mi-iiith':[['/exp1/index.html','/latest_project/bin/magnet1.html'],
	['/exp1/index.html','/latest_project/bin/magnet.html'],
	['/exp2/index.html','/latest_project/bin/magnet1.html'],
	['/exp3/index.html','/latest_project/bin/magnet.html'],
	['/exp4/index.html','/suraj/Suraj/bin/Electrostatic.html'],
	['/exp5/index.html','/suraj/Suraj/bin/Hexane_Vanderwall.html'],
	['/exp6/index.html','/suraj/Suraj/bin/Electrostatic.html'],
	['/exp7/index.html','/suraj/Suraj/bin/Electrostatic_Hexane.html'],
	['/exp8/index.html','/suraj/Suraj/bin/1.html'],
	['/exp9/index.html','/suraj/Suraj/bin/ViscosEnvironment.html'],
	['/exp10/index.html','/suraj/Suraj/bin/oilDrop.html'],
	['/exp11/index.html','/suraj/Suraj/bin/Laser.html']],
'mas-iiith':[['/exp1/index.html','/exp1/media/Principle.html'],
	['/exp1/index.html','/exp1/media/instrument_animation.html'],
	['/exp1/index.html','/exp1/media/1.html'],
	['/exp2/index.html','/exp2/media/2.html'],
	['/exp3/','/exp3/media/length.html'],
	['/exp3/','/exp3/media/3.html'],
	['/exp4/','/exp4/media/concentration.html'],
	['/exp4/','/exp4/media/4.html'],
	['/exp5/index.html','/exp5/media/5.html'],
	['/exp6/','/exp6/media/6.html'],
	['/exp7/','/exp7/media/7.html'],
	['/exp8/','/exp8/media/8.html'],
	['/exp9/','/exp9/media/9.html'],
	['/exp10/','/exp10/media/10.html']],
'mfs-iiith':[['/exp1/index.html','/exp1/media/1.html'],
	['/exp2/index.html','/exp2/media/2_a.html'],
	['/exp2/index.html','/exp2/media/2_b.html'],
	['/exp3/index.html','/exp3/media/3.html'],
	['/exp4/index.html','/exp4/media/4.html'],
	['/exp5/index.html','/exp5/media/5.html'],
	['/exp6/index.html','/exp6/media/6.html'],
	['/exp7/index.html','/exp7/media/7.html'],
	['/exp8/index.html','/exp8/media/8.html'],
	['/exp9/index.html','/exp9/media/9.html'],
	['/exp10/index.html','/exp10/media/10_a.html'],
	['/exp10/index.html','/exp10/media/10_b.html']],
'nlp-iiith':[['/exp1/index.html','/exp1/index.html'],
	['/exp2/index.html','/exp2/index.html'],
	['/exp3/index.html','/exp3/index.html'],
	['/exp4/index.html','/exp4/index.html'],
	['/exp5/index.html','/exp5/index.html'],
	['/exp6/index.html','/exp6/index.html'],
	['/exp7/index.html','/exp7/index.html'],
	['/exp8/index.html','/exp8/index.html'],
	['/exp9/index.html','/exp9/index.html'],
	['/exp10/index.html','/exp10/index.html']],
'cp-iiith':[['/exp1/index.php','/exp1/arithmatic/operator.php'],
	['/exp2/index.php','/exp2/lab/if_else.php'],
	['/exp3/index.php','/exp3/lab/loops.html'],
	['/exp4/index.php','/exp4/index.php'],
	['/exp5/index.php','/exp5/lab/function.html'],
	['/exp6/index.php','/exp6/test1/build/towers.html'],
	['/exp7/index.php','/exp7/structures/build/StrucMain.html'],
	['/exp8/index.php','/exp8/index.php'],
	['/exp9/index.php','/exp9/index.php'],
	['/exp10/index.php','/exp10/Integration/build/factorial.html']],
'cl-iiith':[['/exp1/index.html','/exp1/index.html'],
	['/exp2/index.html','/exp2/index.html'],
	['/exp3/index.html','/exp3/index.html'],
	['/exp4/index.html','/exp4/index.html'],
	['/exp5/index.html','/exp5/index.html'],
	['/exp7/index.html','/exp7/index.html'],
	['/exp8/index.html','/exp8/index.html'],
	['/exp9/index.html','/exp9/index.html'],
	['/exp10/index.html','/exp10/index.html']],
'sd-iiith':[['/exp1/index.html','/exp1/exp1.html'],
	['/exp1/index.html','java.com/en/download/linux_manual.jsp'],
	['/exp2/index.html','/exp2/undamped_freevib.html'],
	['/exp2/index.html','/exp2/dampedfree.html'],
	['/exp2/index.html','java.com/en/download/linux_manual.jsp'],
	['/exp3/index.html','/exp3/undamped_forced.html'],
	['/exp3/index.html','/exp3/DampedForced.html'],
	['/exp3/index.html','java.com/en/download/linux_manual.jsp'],
	['/exp4/index.html','/exp4/Impluse.html'],
	['/exp4/index.html','java.com/en/download/linux_manual.jsp'],
	['/exp5/index.html','/exp5/responsespectrum.avi'],
	['/exp5/index.html','java.com/en/download/linux_manual.jsp'],
	['/exp6/index.html','/exp6/exp5.html'],
	['/exp6/index.html','java.com/en/download/linux_manual.jsp'],
	['/exp7/index.html','/exp7/dem.htm'],
	['/exp7/index.html','java.com/en/download/linux_manual.jsp'],
	['/exp8/index.html','/exp8/torsion6.html'],
	['/exp8/index.html','java.com/en/download/linux_manual.jsp'],
	['/exp9/index.html','/exp9/Coolingtowers.html'],
	['/exp9/index.html','java.com/en/download/linux_manual.jsp'],
	['/exp9/index.html','java.com/en/download/linux_manual.jsp'],
	['/exp9/index.html','java.com/en/download/linux_manual.jsp'],
	['/exp10/index.html','java.com/en/download/linux_manual.jsp']],
'bsa-iiith':[['/exp2/index.php','/exp2/index.php?section=Experiment'],
        ['/exp3/index.php','/exp3/index.php?section=Experiment'],
        ['/exp4/index.php','/exp4/index.php?section=Experiment'],
        ['/exp5/index.php','/exp5/index.php?section=Experiment'],
        ['/exp6/index.php','/exp6/index.php?section=Experiment'],
        ['/exp1/index.php','/exp1/index.php?section=Experiment'],
        ['/exp7/index.php','/exp7/index.php?section=Experiment'],
        ['/exp8/index.php','/exp8/index.php?section=Experiment'],
        ['/exp9/index.php','/exp9/index.php?section=Experiment'],
        ['/exp10/index.php','/exp10/index.php?section=Experiment']],
'ge-iiith':[['/exp1/index.html','/exp1/animation/fsi.html'],
        ['/exp1/index.html','www.oracle.com/technetwork/java/index.html'],
        ['/exp2/index.html','www.oracle.com/technetwork/java/index.html'],
        ['/exp3/index.html','www.oracle.com/technetwork/java/index.html'],
        ['/exp4/index.html','www.oracle.com/technetwork/java/index.html'],
        ['/exp5/index.html','www.oracle.com/technetwork/java/index.html'],
        ['/exp6/index.html','/exp6/media/shearing.html'],
        ['/exp6/index.html','www.oracle.com/technetwork/java/index.html'],
        ['/exp7/index.html','www.oracle.com/technetwork/java/index.html'],
        ['/exp8/index.html','www.oracle.com/technetwork/java/index.html'],
        ['/exp9/index.html','www.oracle.com/technetwork/java/index.html']]}


#+END_SRC
*** Config Script
#+BEGIN_SRC 
import simplejson
import os
def filter():
     
     files_dir = "/home/madhavi/demo/test-filter/"
     
     for file in os.listdir(files_dir):
        key = file.split('_')[1]
        print key
        filtered_file = open(files_dir+file, 'r')
        #print filtered_file
        get_urls=[]
        for line in filtered_file:
             #print line 
             columns = line.split()
             #print columns[1]
             get_urls.append(columns[1])
        #print get_urls
        sim_cont_urls = set(get_urls)
        #print sim_cont_urls
        sim_list = []
        match_list = ["section=Experiment","section=Simulation"]
        for match in match_list:
             for item in sim_cont_urls:
                  if match in item:
                       sim_list.append(item)
        #print sim_list
        cont_list= []
        for item in sim_list:
             #column = item.split('?')
             cont_list.append(item.split('?')[0])
        #print cont_list
        lab_list = []
        #print zip(cont_list,sim_list)
        lab_list = zip(cont_list,sim_list)
        print lab_list

        config_file = open('deploy_config.py','a+')
        config_file.write(key +':')
        simplejson.dump(lab_list,config_file)
    #for item in lab_list:
    #    config_file.write(item)

def csv_to_filtered_labs():
    files_dir = "/home/madhavi/demo/deploy-csv/"
    for file in os.listdir(files_dir):
         lab_csv_file = open(files_dir+file, 'r')
         #print file_name
         filtered_file = "/home/madhavi/demo/test-filter/filtered_%s" % file
         new_file = open(filtered_file, 'w+')
         columns = []
         for line in lab_csv_file:
              columns = line.split(',')
              #print columns[5]
              if(columns[5]!='GET / HTTP/1.1'):
                   new_file.write(columns[5] + "\n")

    filter()

csv_to_filtered_labs()
#+END_SRC 

** Building the Service
   + For building the service, python-flask is used.
   Flask is a micro web application framework written in Python.
   more details about flask can be found [[http://flask.pocoo.org/docs/0.10/][here]].

   + This service, in its present state, will parse the data, extract
   aws-stats, calculate and return the total number of page visits.
   It exposes one endpoint =/numberofhits=.  An "endpoint" is an 
   identifier that is used in determining what logical unit of code
   should handle the request. 

#+BEGIN_SRC python :tangle analytics/app.py
from flask import Flask, render_template, request, jsonify, make_response
# import the flask extension
from flask.ext.cache import Cache
import json
#import os
#import config
#from data import extract_data
#from data.extract_data import *
from extract_data import *
#+END_SRC

Create an instance of the class(Flask). The argument is the name of
the applicationâ€™s module or package.  This is needed so that Flask
knows where to look for templates, static files, and so on.

#+BEGIN_SRC python :tangle analytics/app.py
app = Flask(__name__)
#+END_SRC

  Define the cache config keys.

#+BEGIN_SRC python :tangle analytics/app.py
app.config['CACHE_TYPE'] = 'simple'
#+END_SRC

  Register the cache instance and bind it to app. 

#+BEGIN_SRC python :tangle analytics/app.py
app.cache = Cache(app)
#+END_SRC

  Register the cache instance and bind it to app. 

Use the =route()= decorator to tell Flask what URL should trigger the
function.

#+BEGIN_SRC python :tangle analytics/app.py
@app.route('/numberofhits')
#+END_SRC

 Cache this view for 1 hour. This is needed because the service must
 display the updated count every one hour.

#+BEGIN_SRC python :tangle analytics/app.py
@app.cache.cached(timeout=360)  
#+END_SRC

 The function is given a name which is also used to generate URLs
 for that particular function, and returns the value we want to
 display in the browser.
 
#+BEGIN_SRC python :tangle analytics/app.py
def numberofhits():
#+END_SRC

 Call the function =grand_total()= which is defined in
 =extract_data.py= to get the total number of hits.
  
#+BEGIN_SRC python :tangle analytics/app.py
    numberofhits  = grand_total()
#+END_SRC

   The =make_response()= function can be called instead of using a return
   and will get a response object which can be used to attach headers.
   =Access-Control-Allow-Origin= is a CORS (Cross-Origin Resource
   Sharing) header.  When Site A tries to fetch content from Site B,
   Site B can send an =Access-Control-Allow-Origin= response header to
   tell the browser that the content of this page is accessible to
   certain origins.  By default, Site B's pages are not accessible to
   any other origin, using the =Access-Control-Allow-Origin= header
   opens a door for cross-origin access by specific requesting
   origins.  The server can give permission to include cookies by
   setting the =Access-Control-Allow-Credentials= header.

#+BEGIN_SRC python :tangle analytics/app.py
    response = make_response(numberofhits)
    response.headers['Access-Control-Allow-Origin'] = '*'
    response.headers['Access-Control-Allow-Credentials'] = 'true'
    return response
#+END_SRC
   Similary, usage count will get from the following end point.
#+BEGIN_SRC python :tangle analytics/app.py
@app.route('/usage')
@app.cache.cached(timeout=360)  
def usagehits():
    usage_count  = usage_hits()
    response = make_response(usage_count)
    response.headers['Access-Control-Allow-Origin'] = '*'
    response.headers['Access-Control-Allow-Credentials'] = 'true'
    return response
#+END_SRC

   Finally, use the =run()= function to run the local server with
   our application. The if *__name__ == '__main__':* makes sure the
   server only runs if the script is executed directly from the Python
   interpreter and not used as an imported module.  

#+BEGIN_SRC python :tangle analytics/app.py
if __name__ == '__main__':
    app.run(port=5000, debug=True)
#+END_SRC
*** Provisioning the Service
***** Machine Configuration
   + Operating System: centos-6.6
   + Architecture: x86_64
   + Memory: 256 MB
   + Disk space: 10 GB
   + Interface: venet0
***** Steps to manually create a centos container 
#+BEGIN_SRC 
vzctl create 16133 --ostemplate centos-6-x86_64-point6 --ipadd 10.4.15.133 
--diskspace 10G:15.0G --hostname stats-demo.vlabs.ac.in
vzctl start 16133
vzctl set 16133 --nameserver inherit --ram 256M --swap 512M --onboot yes --save
#+END_SRC
***** Export proxy Settings
#+BEGIN_SRC 
export http_proxy="proxy.iiit.ac.in:8080"
export https_proxy="proxy.iiit.ac.in:8080"
#+END_SRC    
***** Update the System
    In order to have a stable deployment server, it is crucial to keep things up-to-date and well maintained.
    To ensure that we have the latest available versions of default applications, we need to update our system.
    Run the following command to update your system
#+BEGIN_SRC 
sudo yum -y update
#+END_SRC
***** Install virtualenv
    Run the following command to download and install virtualenv using pip.
    =virtualenv= is a tool to create isolated Python environments.
#+BEGIN_SRC 
sudo pip install virtualenv
#+END_SRC
***** Install epel For RHEL 6.x and CentOS 6.x (x86_64)
#+BEGIN_SRC 
rpm -ivh http://dl.fedoraproject.org/pub
/epel/6/x86_64/epel-release-6-8.noarch.rpm
#+END_SRC
***** Install pip with yum command
#+BEGIN_SRC 
yum install -y python-pip
#+END_SRC
***** Install Flask
   Enter the following command to get Flask activated in your =virtualenv=
#+BEGIN_SRC 
 pip install Flask
#+END_SRC
***** Install Flask-Cache
#+BEGIN_SRC 
 pip install Flask-Cache
#+END_SRC
***** Install wsgi on CentOS using yum
   WSGI(Web Server Gateway Interface) is an interface between a web server 
   and the application itself. It exists to ensure a standardized way 
   between various servers and applications (frameworks) to work with each 
   other, allowing interchangeability when necessary (e.g. switching from 
   development to production environment).
#+BEGIN_SRC 
yum install mod_wsgi
#+END_SRC

***** Create a =.wsgi= file
    To run your application you need a =analytics.wsgi= file. 
    This file contains the code =mod_wsgi= is executing on startup to get the application object. 
    The object called application in that file is then used as application.
#+BEGIN_SRC  python :tangle analytics/analytics.wsgi
import sys
sys.path.insert (0,'/var/www/html/analytics/')

import logging, sys
logging.basicConfig(stream=sys.stderr)

from app import app as application
 #+END_SRC
***** Configure the Apache and deploy the service
      Configure Apache to load =mod_wsgi= module and your project in VirtualHost
      Insert the following lines in =/etc/httpd/conf/httpd.conf=
#+BEGIN_SRC 
WSGIScriptAlias / /var/www/html/analytics/analytics.wsgi
WSGIScriptReloading On
<Directory /var/www/html/analytics>
     Order deny,allow
     Allow from all
 </Directory>

#+END_SRC
***** Restart Apache
#+BEGIN_SRC 
service httpd restart
#+END_SRC
***** Test the service with end point.
#+BEGIN_SRC 
http://10.4.15.133/numberofhits
#+END_SRC

** Presenting the data
   For presenting the data, AJAX (Asynchronous JavaScript and
   XML)and HTML(HyperText Markup Language) is used. 
   AJAX allows web pages to be updated asynchronously by
   exchanging small amounts of data with the server. This means that
   it is possible to update parts of a web page, without reloading the
   whole page. The code snippet below will call the endpoint
   =/numberofhits= and =/usage=, where it will return the total number of pages 
   visited and usage as a response and displays on browser.

#+BEGIN_SRC html :tangle analytics/html-code/index.html 
<html>
   <head>
      <title>Analytics</title>
      <script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
      <script type="text/javascript" language="javascript">
         //$(document).ready(function() {
         window.onload = function() {
               $.getJSON('http://stats-demo.vlabs.ac.in/numberofhits', function(data) {
                  $("#analytics").html("<p><b>Page hits since 2013: " + data + "</b></p>");
                  });            
                  $.getJSON('http://stats-demo.vlabs.ac.in/usagehits', function(data) {
                  $("#usage").html("<p><b>Usage hits since 2013: " + data + "</b></p>");

              });
         };
      </script>
     </head>
   <body>
      <div id="analytics">
         Analytics
      </div>
      <div id="usage">
         Usage
      </div>
   </body>
</html>
#+END_SRC

   Note: If you want to display the total number of page visits and usage on vlab.co.in, 
   then add the above code snippet in =index.html= file. 
* Impediments
** Variety in URL patterns
   While populating the config files with the experiment URLs and
   Simulation patterns, it was observed that all the labs did not
   follow a particular naming convention for their
   experiments. Different labs had different naming conventions, which
   made the automation of extracting the URL and Simulation pattern
   from the logs difficult.  The extraction of the patterns from the
   log files is done manually.
** Separate logs for same labs
   Apache generates an access log for each lab. Logrotate is is
   triggered by Apache to digest the logs upto a certain limit
   (ie. file size) into another file and tag it with the date when the
   digest was made. Apache is reloaded once this change is
   made. Because of this mechanism, each lab has several access logs
   associated. All these logs have to be considered while calculating
   the usage. All these logs should be collated into on single place
   before matching the experiment and simulation patterns.
** Different request for experiment/simulation
   There were separate requests for the experiment and the
   simulation. The experiment pattern was not a substring of the
   simulation pattern. A separate module/function has to be written to
   calculate usage for such labs. The patterns of such labs are put
   into another config file. This would help in differentiating the
   different types of labs.

** Availability of the log files
   The log files are collated from two different locations, AWS and
   IIIT infrastructure. The logs from both the locations are
   necessary to calculate the usage count. The logs are generally
   rotated after a period of 4 weeks on AWS. This period has to be
   increased to retain the logs for a longer duration. The logs on
   the local infrastructure are set to 52 weeks (ie a year).

* Test Cases
  A test case is a document, which has a set of test data,
  preconditions, expected results and post-conditions, developed for a
  particular test scenario in order to verify compliance against a
  specific requirement.
** Objective:
   The objective of this test cases is to test whether the application 
   is running or not, by sending HTTP GET request and also to test 
   the return value of the response. 
** Test case ID: TC01
*** Test case name: Test Number of Page Visits
*** Test case description:
    In this test case, end point =/numberofhits= is tested, where it 
    will return the total number of pages visited.
*** Test data/Input data:
   The Input data is required to test the test case. 
   Input data is to verify that a given set of 
   input to a given function/program produces some expected result. 
   Input can be valid data or invalid data.
   Here, the input data will return the user-defined value when
    =grand_total()= function is called. 
#+BEGIN_SRC python :tangle analytics/tests/sample_data.py
#!/usr/bin/python
def grand_total():
  dict = {'numberofhits':'1234'};
  return dict['numberofhits']
#+END_SRC
*** Step description/action
**** Step 1:
    Import the required packages used for testing.
#+BEGIN_SRC python :tangle analytics/tests/test_app.py
import unittest
from flask.ext.testing import TestCase
import sample_data
from app import app
#+END_SRC
**** Step 2:
 A test case is created by sub-classing =unittest.TestCase=.
#+BEGIN_SRC python :tangle analytics/tests/test_app.py
class AnalyticsTestCase(unittest.TestCase):
#+END_SRC
**** Step 3:
    The tests are defined with methods whose names start with the
    letters =test=.  This naming convention informs the test runner
    about which methods represent tests.  To start testing the
    functionality of the application, add a new test method to our
    class, like =test_numberofhits()=.Similarly, other test methods
    can be added. 
#+BEGIN_SRC python :tangle analytics/tests/test_app.py 
    def test_numberofhits(self):
        json_string = sample_data.grand_total()
        print json_string
#+END_SRC
**** Step 4:
   Each test is a call to =assertEqual()= function which will test
   that first and second arguments are equal.  If the values do not
   compare equal, then test will fail.
   - The status code of the endpoint =/numberofhits= is equated to =200= 
   which means =OK= (the request has succeeded). The endpoint returned 
   with the response.
   - The data which is returned in the form of response is compared against
     the Input data.
#+BEGIN_SRC python :tangle analytics/tests/test_app.py 
        tester = app.test_client(self)
        response = tester.get('http://localhost:5000/numberofhits')
        self.assertEqual(response.status_code, 200)
        self.assertEqual(json_string,  response.data)

#+END_SRC            
**** Step 5:
    The final block shows a simple way to run the
    tests. =unittest.main()= provides a command-line interface to the
    test script.  When run from the command line, the script
    produces an output.
#+BEGIN_SRC  
if __name__ == '__main__':
    unittest.main()
#+END_SRC

*** Expected Result
    - Run =python test_app.py= in terminal.
    - Check the output, if it was like =Ran 1 test in 0.034s= and =OK=.
    - If the output is found then test is passed.
 #+BEGIN_SRC 
$ python test_app.py
.
----------------------------------------------------------------------
Ran 1 test in 0.034s

OK
#+END_SRC

** Test case ID: TC02
*** Test case name: Test Usage 
*** Test case description:
    In this test case, end point =/usage= is tested, where it 
    will return the total number of usage.
*** Test data/Input data:
   The Input data is required to test the test case. 
   Input data is to verify that a given set of 
   input to a given function/program produces some expected result. 
   Input can be valid data or invalid data.
   Here, the input data will return the user-defined value when
    =usage_hits()= function is called. 
#+BEGIN_SRC python :tangle analytics/tests/sample_data.py 
def usage_hits():
  dict = {'numberofusagehits':'1234'};
  return dict['numberofusagehits']
#+END_SRC
*** Step description/action
**** Step 1:
    Import the required packages used for testing.
#+BEGIN_SRC  
import unittest
from flask.ext.testing import TestCase
import sample_data
from app import app
#+END_SRC
**** Step 2:
 A test case is created by sub-classing =unittest.TestCase=.
#+BEGIN_SRC 
class AnalyticsTestCase(unittest.TestCase):
#+END_SRC
**** Step 3:
    The tests are defined with methods whose names start with the
    letters =test=.  This naming convention informs the test runner
    about which methods represent tests.  To start testing the
    functionality of the application, add a new test method to our
    class, like =test_usage()=.Similarly, other test methods
    can be added. 
#+BEGIN_SRC python :tangle analytics/tests/test_app.py 
    def test_usage(self):
        json_string = sample_data.usage_hits()
        print json_string
#+END_SRC
**** Step 4:
   Each test is a call to =assertEqual()= function which will test
   that first and second arguments are equal.  If the values do not
   compare equal, then test will fail.
   - The status code of the endpoint =/numberofhits= is equated to =200= 
   which means =OK= (the request has succeeded). The endpoint returned 
   with the response.
   - The data which is returned in the form of response is compared against
     the Input data.
#+BEGIN_SRC python :tangle analytics/tests/test_app.py
        tester = app.test_client(self)
        response = tester.get('http://localhost:5000/usage')
        self.assertEqual(response.status_code, 200)
        self.assertEqual(json_string,  response.data)

#+END_SRC            
**** Step 5:
    The final block shows a simple way to run the
    tests. =unittest.main()= provides a command-line interface to the
    test script.  When run from the command line, the script
    produces an output.
#+BEGIN_SRC python :tangle analytics/tests/test_app.py
if __name__ == '__main__':
    unittest.main()
#+END_SRC

*** Expected Result
    - Run =python test_app.py= in terminal.
    - Check the output, if it was like =Ran 1 test in 0.034s= and =OK=.
    - If the output is found then test is passed.
 #+BEGIN_SRC 
$ python test_app.py
.
----------------------------------------------------------------------
Ran 2 tests in 0.022s

OK

#+END_SRC

* Releases
** Release v1.0.0
   This release realizes [[Requirement #1]].
   The release date is [2015-07-08 Wed]
*** Work Plan
**** DONE Prepare the document
**** DONE Discuss on how to meet the requirement
          There was a discussion on [2015-06-23 Tue] with the
          participants as Saurabh, Thirumal, Anon, Zubair, Soumya and
          the other VLEAD members. The task division and the process to
          bring up the service was also discussed which is mentioned in
          the [[Process%20for%20implementation][Process for implementation]] section.
**** DONE Figure out the various location of the labs
          There are several locations the labs are situated namely AWS,
          deploy, separate containers. How to display the combined
          output of all the labs is yet to be decided.  (This has been
          done and the decision taken has been mentioned in the [[Present%20scenario%20of%20statistics][Present
          scenario of statistics]])

**** DONE Build the scripts
**** DONE Test the scripts.
**** DONE Updation of page view count in new landing page of vlab.co.in

** Release v1.0.2
   This release realizes [[Requirement%20#2][Requirement #2]].
   The release date is: [2015-08-01 Sat]
*** Work Plan
**** TODO Update model with usage design [[https://github.com/vlead/analytics/issues/2][#2]]
**** TODO Write script  to convert logs into csv format [[https://github.com/vlead/analytics/issues/9][#9]]
**** TODO Write a python script to extract usage form the CSV formatted logs. [[https://github.com/vlead/analytics/issues/8][#8]]
**** TODO Update model with usage extraction script implementation [[https://github.com/vlead/analytics/issues/12][#12]]
**** TODO Add a new endpoint to the service [[https://github.com/vlead/analytics/issues/6][#6]]
**** TODO A display page to show the usage [[https://github.com/vlead/analytics/issues/7][#7]]
**** TODO Update model with html, ajax and js implementation [[https://github.com/vlead/analytics/issues/13][#13]]
**** TODO Update model with service implementation [[https://github.com/vlead/analytics/issues/3][#3]]
**** TODO Write test cases [[https://github.com/vlead/analytics/issues/5][#5]]
**** TODO Update model with test-cases [[https://github.com/vlead/analytics/issues/4][#4]]
**** TODO Add usage definitions to pop-up page [[https://github.com/vlead/analytics/issues/14][#14]]
**** DONE Add usage definitions to pop-up page [[https://github.com/vlead/analytics/issues/14][#14]]
*** Estimated Effort
 - Start on:20/07/2015
 - End on:31/07/2015 
 - Release Date: 01/08/2015
 - Estimated Time: 120 Person Hours
 - Hours in day = 6
 - Total days = 120/6 = 20
 - Persons = 2
 - Clock days = 20/2 = 10 
*** Actual Effort
   Available hours(from 20/07/2015 to 05/08/2015):
   - Soumya = 72 hours 
   - Madhavi = 72 hours 
   Time spent in meetings:
   - Systems Meeting: Soumaya - 3 hours
   - Data Services Meeting: Madhavi - 3 hours
   - Weekly status update Meeting(29/07/2015): Soumya, Madhavi = 3+3=6 Person Hours
   - Weekly status update Meeting(05/08/2015): Soumya, Madhavi = 3+3=6 Person Hours
  
   Actual Time:
   - Soumya = 72-12 = 60 Hours
   - Madhavi =72 -12 = 60 Hours




* COMMENT  introduction
  The analytics of Virtual-labs are currently generated by awstats,
  which is an on-site web analytics technology. Awstats provides its
  own dashboard for viewing statistics.  It provides a layman with too
  much information to comprehend.  A simple dashboard which displays
  the total views and hits of the site is required. This would help
  keep us informed on the usage of our labs.

* COMMENT Design
   The aim of this model is to build a web service which
   would display certain specific information from the statistics
   generated by awstats. As the diagram suggests, a script will be
   embedded on vlab.co.in, which contains the URL of the service. This
   would invoke the service running on our server which would display
   the total number of pages. The service in turn requires the parsed
   files generated by awstats. These files need to be periodically
   updated to obtain the latest data from the logs. From different
   locations, these files are transferred into the service machine.
  
* COMMENT Implementation
** Procedure
   After a discussion (which also involved exchange of mails) the
   process of implementation was decided as follows:
   1. Pull all the data from deploy and AWS
      - Collate data from deploy and AWS
      - This data that will be used by the service using helper functions.
   2. Serving the data. 
      - Define end points based on the requirements
      - Build a service that implements these end points.
   3. Rendering of the data on vlab.co.in
      - Insert the JavaScript snippet in vlab.co.in
   4. Backup of all the statistics
       
  A service needs to run which would invoke the script written above
  periodically to obtain the latest data. Also the service is required
  to display the data on a html page.
  
  *Note*: Currently the HTML page is on vlab.co.in home page. In
  future a button / hyperlink could also be incorporated which would
  display a page that shows slightly more details.
    
