#+TITLE:     Building an application for the Analytics of Virtual Labs
#+AUTHOR:    M.S.Soumya
#+DATE:      2015-06-09 Tue
#+PROPERTY: results output
#+PROPERTY: exports code
#+SETUPFILE: org-templates/level-0.org

* Introduction
  Web analytics is used to measure the web traffic which helps in
  knowing the site visitor count and several others values like the
  number of page views etc. This model describes a simple web
  application which would display the analytics of virtual-labs.

* Requirements
** Current requirement
   1. The visitor page views count from both AWS and local deployment
      must be displayed on vlab.co.in
   2. The service must display the updated count every one hour.
** Future requirement
   The analytics of virtual-labs can be portrayed in terms of usage
   also.  Usage can be defined as: 

   - 1 usage (instance) :: loading content of 1 experiment + loading
                           simulation of 1 experiment
   
   From the definition of usage we can collect a lot of information
   eg.
   + The total usage of virtual-labs
   + The total number of experiments/
   + Labwise analytics
   + Geographical analytics   

* Design
** Design decisions
*** Collation of statistics
    The statistics are present in two different locations AWS and on
    the local reverseproxy at VLEAD. To get numbers the data is
    required from both these locations. It was decided that on an AWS
    VM the data from both locations would be collated. The awstats
    data will be transferred on an hourly basis to this VM. 

*** Where the service will run
    There were concerns on where the service should run. Initially it
    was being assumed that the service would run on the reverse proxy
    server itself. This was so because the stats file were located on
    this machine. By doing so it would reduce the overhead of
    transferring the stats files into another location and also the
    data that the service would use might be stale.  Setting up the
    service on the reverse proxy had its own security threats.  So,
    after a lot of discussion it was decided that the service will run
    on a separate VM on AWS.
*** Setup of the service
    As discussed in the [[Where%20the%20service%20will%20run][Where the service will run]], a VM on AWS will
    be created to run the service. The VM will be a part of the AWS
    cluster. The concern in setting up this VM is that the other VM's
    in the cluster are setup by ansible scripts. To setup this service
    ansible scripts need to be written. Manual entry of the FQDN and
    IP into some servers like reverse proxy and DNS may work
    temporarily but when the scripts are run, this information will be
    erased.  *Note*: Yet to be discussed.

** Design description
   The aim of this model is to build a web service which would display
   certain specific information from the statistics generated by
   awstats. As the diagram suggests, a script will be embedded on
   vlab.co.in, which contains the URL of the service. This would
   invoke the service running on our server which would display the
   total number of pages. The service in turn requires the parsed
   files generated by awstats. These files need to be periodically
   updated to obtain the latest data from the logs. From different
   locations, these files are transferred into the service machine.
  
  #+CAPTION:  Design diagram
  #+LABEL:  Design diagram
  [[./diagrams/analytics.png]]

   For simpler understanding the design is broken down into 3 parts:
   + collation of data
   + extraction of data
   + a service to present data

*** Collation of data
    Initially all the labs were hosted from VLEAD infrastructure.  For
    scalability the hosting of labs shifted to AWS. The statistics for
    both were being collected. Later the deploy container was
    decommissioned and certain labs were moved to individual
    containers. More details about the deploy decommissioning project
    can be found [[https://bitbucket.org/vlead/vlead/src/8849aa7d1a44b25dee4ac1b88f4c5f4327e8f2a1/projects/cleaning-deploy/?at%3Dmaster][here]].

    Currently the labs are hosted in 3 different locations namely:
    + AWS ( Currently 63 labs)
    + Containers at VLEAD (Around 20 labs) 
    + Rest of the labs directed by vlab.co.in (either hosted by Amrita
      or individual institutes)
    
    The statistics of the labs hosted on AWS and the containers at
    VLEAD can be obtained because Awstats was configured and the
    statistics files are accessible. The statistics of the rest of the
    labs which are directed by vlab.co.in (not on AWS or on the VLEAD
    containers) cannot be obtained.

    #+CAPTION:  data collation design diagram
    #+LABEL:  data collation design diagram
    [[./diagrams/file-flow-diagram.png]]
    
*** Extraction of data
    Awstats generates parsed text files which contain the statistics
    in text format. The required data from these text files needs to
    be extracted and displayed on a HTML page. 

    A script has to be written which would do the following:
    + parse the awstats data files
    + extract the =number of pages viewed= from each data file
    + Sum up all such figures from all the files in AWS
    + Sum up all such figures from the specific files in VLEAD
    + Total the number of pages viewed from AWS and VLEAD.

*** A service to present data   
    A service needs to run which would invoke the script written above
    periodically to obtain the latest data. Also the service is
    required to display the data on a html page.

    *Note*: Currently the HTML page is on vlab.co.in home page. In
    future a button / hyperlink could also be incorporated which would
    display a page that shows slightly more details.
    
* Implementation
** Procedure
   After a discussion (which also involved exchange of mails) the
   process of implementation was decided as follows:
   1. Pull all the data from deploy and AWS
      - Collate data from deploy and AWS
      - This data that will be used by the service using helper functions.
   2. Serving the data. 
      - Define end points based on the requirements
      - Build a service that implements these end points.
   3. Rendering of the data on vlab.co.in
      - Insert the JavaScript snippet in vlab.co.in
   4. Backup of all the statistics
      
** Impediments
   
** Data collation

   We have two different sources, a reverse proxy on AWS
cluster(Source 1) and a reverse proxy on the base machines in IIIT-H
(Source 2). These sources are geographically located in different
continents and are on different networks.

   We need a way of transferring the data(statistics) to the analytics
server where it will be processed. We have used =rsync= to transfer,
and have setup periodic transfer jobs using =cronie= which executes
every hour.

As this task involves collecting data from different sources, like a
cluster which is managed with configuration management tool =Ansible=
and another one a manually configured server, data collation is also a
mix of automation and manual steps.

We now present the configuration procedure of both.

*** Re-configuring AWS cluster, requirements of analytics node
 - Analytics server configured to accept =rsync= over =TCP=
 - Reverse Proxy(Source 1) configured to push the generated statistics
   to =Analytics= server at regular intervals
 - Router cofigured to allow incoming =rsync= connections from Source
   2 to Analytics server
 
 As AWS cluster is configured by =Ansible=, we have scripts to achieve
 the above via =Ansible=.

*** On BASE
 Here we manually setup:
 - Reverse Proxy(Source 2) configued to push generated statistics to
   =Analytics= server every hour

*** TODO
 -  Find a way to rsync after/before awstats update script, so the
    chances of file corruption are minimized.

** Data extraction
   The code below is written to extract certain page views from the
   awstats text files. The purpose of the script is described in
   [[Extraction%20of%20data][Extraction of data]]. This script has to be placed in the same
   directory as the data files.

#+BEGIN_SRC python :tangle extract_data.py
import sys
import re
import json
import os
import fnmatch

def get_file_names(year):
    list_of_files = []
    for file in os.listdir('.'):
        if fnmatch.fnmatch(file, '*'+ re.escape(year) + '*.txt'):
            list_of_files.append(file)
    return list_of_files

def extract_aws_data(year):
    files = get_file_names(year);
    dictionary = {}
    dict_tot_pages = {}
    for i in files:
        f = open(i)
        for line in f:
            if re.match(r'^BEGIN_DAY',line):
                n = int((line.strip('BEGIN_DAY ')).rstrip())
                x = 0
                p = 0
                for page_line in f:
                    if (x >= n):
                        break
                    p += int(page_line.split()[1])
                    x += 1
                f.close()
                dictionary[(i.lstrip('awstats.')).rstrip('.txt')] = p
                break
    z = 0
    for key in dictionary:
        z += dictionary[key]
    return z

def extract_iiit_data(year):
    files = get_file_names(year);
    dictionary = {}
    dict_tot_pages = {}
    for i in files:
        if re.search(r'awstats\d+.virtual-labs.ac.in.txt$',i):
            f = open(i)
            for line in f:
                if re.match(r'^BEGIN_DAY',line):
                    n = int((line.strip('BEGIN_DAY ')).rstrip())
                    x = 0
                    p = 0
                    for page_line in f:
                        if (x >= n):
                            break
                        p += int(page_line.split()[1])
                        x += 1
                    f.close()
                    dictionary[(i.lstrip('awstats.')).rstrip('.txt')] = p
                    break
    z = 0
    for key in dictionary:
        z += dictionary[key]
    return z

def grand_total():
    year = ['2013', '2014', '2015' ]
    tot_pages=0
    for i in year:
        tot_pages += extract_iiit_data(i) + extract_aws_data(i)
    dict_tot_pages = {}
    dict_tot_pages["total-pages"]=tot_pages
grand_total()
#+END_SRC
** Building the Service
   The service exposes all the end points. currently it exposes one endpoint =/numberofhits=. 
   This service will parse the data, extract aws-stats, calculate and returns the total number of page visits.

#+BEGIN_SRC python :tangle app.py
from flask import Flask, make_response
import json
import os
#import config
from data import extract_data
# import the flask extension
from flask.ext.cache import Cache
#+END_SRC

Create an instance of the class(Flask). The argument is the name of the application’s module or package. 
This is needed so that Flask knows where to look for templates, static files, and so on. 

#+BEGIN_SRC 
app = Flask(__name__)
#+END_SRC

  Define the cache config keys.

#+BEGIN_SRC
app.config['CACHE_TYPE'] = 'simple'
#+END_SRC

  Register the cache instance and bind it to app 

#+BEGIN_SRC 
app.cache = Cache(app)
#+END_SRC

Use the =route()= decorator to tell Flask what URL should trigger the function.

#+BEGIN_SRC 
@app.route('/numberofhits')
#+END_SRC

 Cache this view for 1 hour. This is needed because the service must display the updated count every one hour.

#+BEGIN_SRC 
@app.cache.cached(timeout=360)  
#+END_SRC

 The function is given a name which is also used to generate URLs
 for that particular function, and returns the value we want to
 display in the browser.
 
#+BEGIN_SRC 
def numberofhits():
#+END_SRC

 Call the function =grand_total()= which is defined in
 =extract_data.py= to get the total number of hits.
  
#+BEGIN_SRC 
    numberofhits  = extract_data.grand_total()
#+END_SRC

   The =make_response()= function can be called instead of using a return
   and will get a response object which can be used to attach headers.
   =Access-Control-Allow-Origin= is a CORS (Cross-Origin Resource
   Sharing) header.  When Site A tries to fetch content from Site B,
   Site B can send an =Access-Control-Allow-Origin= response header to
   tell the browser that the content of this page is accessible to
   certain origins.  By default, Site B's pages are not accessible to
   any other origin, using the =Access-Control-Allow-Origin= header
   opens a door for cross-origin access by specific requesting
   origins.  The server can give permission to include cookies by
   setting the =Access-Control-Allow-Credentials= header.

#+BEGIN_SRC
    response = make_response(numberofhits)
    response.headers['Access-Control-Allow-Origin'] = '*'
    response.headers['Access-Control-Allow-Credentials'] = 'true'
    return response
#+END_SRC

   Finally, use the =run()= function to run the local server with
   our application. The if *__name__ == '__main__':* makes sure the
   server only runs if the script is executed directly from the Python
   interpreter and not used as an imported module.  

#+BEGIN_SRC 
if __name__ == '__main__':
    app.run(port=5000, debug=True)
#+END_SRC
   
** Presenting the data
   For presenting the data, we used AJAX (Asynchronous JavaScript and XML)
   AJAX allows web pages to be updated asynchronously by exchanging 
   small amounts of data with the server. This means that it is possible to 
   update parts of a web page, without reloading the whole page. The code snippet
   below will call the enpoint =/numberofhits=, where it will return the total number of pages
   visited as a response and displays on browser.

#+BEGIN_SRC 
<html>
   <head>
      <title>Analytics</title>
      <script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
      <script type="text/javascript" language="javascript">
         $(document).ready(function() {
               $.getJSON('http://localhost:5000/numberofhits', function(data) {
                  $("#analytics").html("<p> Total number of hits: " + data.numberofhits + "</p>");
               });
         });
      </script>
   </head>
   <body>
      <div id="analytics">
         Analytics
      </div>
   </body>
</html>
#+END_SRC

   Note: If you want to display the total number of page visits on vlab.co.in, 
   then Add the above code snippet in =index.html= file. 

* Test Cases
  A test case is a document, which has a set of test data,
  preconditions, expected results and postconditions, developed for a
  particular test scenario in order to verify compliance against a
  specific requirement.
** Objective:
   The objective of this test cases is to test whether the application 
   is running or not, by sending HTTP GET request and also to test 
   the return value of the response. 
** Test case ID: TC01
** Test case name: Test Number of Page Visits
** Test case description:
   In this test case, end point =/numberofhits= is tested, where it 
   will return the total number of pages visited.
** Pre condition:
    Service is build and endpoint is exposed.
** Test data/Input data:
   The Input data is required to test the test case. 
   Input data is to verify that a given set of 
   input to a given function/program produces some expected result. 
   Input can be valid data or invalid data.
   Here, the input data will return the userdefined value when
    =grand_toal()= function is called. 
#+BEGIN_SRC python:tangle config.py
#!/usr/bin/python
def grand_total():
  dict = {'numberofhits':'1234'};
  return dict['numberofhits']
#+END_SRC
** Step description/action
*** Step 1:
    Import the required packages used for testing.
#+BEGIN_SRC python :tangle test_app.py
import unittest
from flask.ext.testing import TestCase
import config
from app import app
#+END_SRC
*** Step 2:
 A testcase is created by subclassing =unittest.TestCase=.
#+BEGIN_SRC 
class AnalyticsTestCase(unittest.TestCase):
#+END_SRC
*** Step 3:
    The tests are defined with methods whose names start with the
    letters =test=.  This naming convention informs the test runner
    about which methods represent tests.  To start testing the
    functionality of the application, add a new test method to our
    class, like =test_numberofhits()=.Similarly, other test methods
    can be added. 
#+BEGIN_SRC 
    def test_numberofhits(self):
        json_string = config.grand_total()
        print json_string
#+END_SRC
*** Step 4:
   Each test is a call to =assertEqual()= function which will test
   that first and second arguments are equal.  If the values do not
   compare equal, then test will fail.
   - The status code of the endpoint =/numberofhits= is equated to =200= 
   which means =OK= (the request has succeeded). The endpoint returned 
   with the response.
   - The data which is returned in the form of response is compared against
     the Input data.
#+BEGIN_SRC 
        tester = app.test_client(self)
        response = tester.get('http://localhost:5000/numberofhits')
        self.assertEqual(response.status_code, 200)
        self.assertEqual(json_string,  response.data)

#+END_SRC            
*** Step 5:
    The final block shows a simple way to run the
    tests. =unittest.main()= provides a command-line interface to the
    test script.  When run from the command line, the script
    produces an output.
#+BEGIN_SRC 
if __name__ == '__main__':
    unittest.main()
#+END_SRC

** Expected Result
    - Run =python test_app.py= in terminal.
    - Check the output, if it was like =Ran 1 test in 0.034s= and =OK=.
    - If the output is found then test is passed.
 #+BEGIN_SRC 
$ python test_app.py
.
----------------------------------------------------------------------
Ran 1 test in 0.034s

OK
#+END_SRC


* Work Plan
** Work plan for simple application
*** TODO Prepare the document
*** DONE Discuss on how to meet the requirement
         There was a discussion on [2015-06-23 Tue] with the
         participants as Saurabh, Thirumal, Anon, Zubair, Soumya and
         the other VLEAD members. The task division and the process to
         bring up the service was also discussed which is mentioned in
         the [[Process%20for%20implementation][Process for implementation]] section.
*** DONE Figure out the various location of the labs
         There are several locations the labs are situated namely AWS,
         deploy, separate containers. How to display the combined
         output of all the labs is yet to be decided.  (This has been
         done and the decision taken has been mentioned in the [[Present%20scenario%20of%20statistics][Present
         scenario of statistics]])

*** DONE Build the scripts
*** DONE Test the scripts.
*** TODO Updation of vlab.co.in


* COMMENT  introduction
  The analytics of Virtual-labs are currently generated by awstats,
  which is an on-site web analytics technology. Awstats provides its
  own dashboard for viewing statistics.  It provides a layman with too
  much information to comprehend.  A simple dashboard which displays
  the total views and hits of the site is required. This would help
  keep us informed on the usage of our labs.

* COMMENT Objective
  The objective of this exercise is to display the usage of
  virtual-labs in terms of number of pages viewed on vlab.co.in.
   
