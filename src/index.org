#+TITLE:     Building a basic service to display the Analytics of Virtual Labs
#+AUTHOR:    M.S.Soumya
#+DATE:      2015-06-09 Tue
#+PROPERTY: results output
#+PROPERTY: exports code
#+SETUPFILE: org-templates/level-0.org

*NOTE:* This document is not yet complete though it is committed ! It
is being worked on continuously.

* Problem
  The analytics of Virtual-labs are currently generated by
  awstats. Awstats provides a dashboard for viewing these statistics.
  It provides a layman with too much information to comprehend.  A
  simple dashboard which displays the total views and hits of the site
  is required. This would help keep us informed on the usage of our
  labs.

* Objective
  The objective of this exercise is to display the usage of
  virtual-labs in terms of number of pages viewed on vlab.co.in.

* Plan
  #+CAPTION:  Design diagram
  #+LABEL:  Design diagram
  [[./diagrams/analytics.png]]
 
  To build a web service which would display certain specific
  information from the statistics generated by awstats. As the diagram
  suggests, this service would have a URL which would be hyper-linked
  to a button on the vlab.co.in page.  On clicking the button this
  service would get invoked and the information is displayed on the
  page. This would be a dynamic process and any changes in the
  statistics must be incorporated by the service.

* Implementation
** Requirement
   The pages views count from both AWS and local deployment must be
   displayed on vlab.co.in
** Background
   Awstats generates parsed text files which contain the statistics in
   text format. The required data from these text files needs to be
   extracted and displayed on a HTML page.To achieve this we build the
   service.  

   *Note*: Currently the HTML page is on vlab.co.in home page. In
   future a button / hyperlink could also be incorporated which would
   display a page that shows slightly more details.

** Present status of statistics
   Currently the labs are hosted in 3 different locations namely:
   + AWS ( Currently 63 labs)
   + Containers at VLEAD (Around 20 labs) 
   + Rest of the labs directed by vlab.co.in (either hosted by Amrita
     or individual institutes)
   
   The statistics of the labs hosted on AWS and the containers at
   VLEAD can be obtained because Awstats was configured and the
   statistics file are accessible. The statistics of the rest of the
   labs which are directed by vlab.co.in (not on AWS or on the VLEAD
   containers) cannot be obtained.

** Decisions
*** Collation of statistics
    The statistics are present in two different locations AWS and on
    the local reverseproxy at VLEAD. To get numbers the data is
    required from both these locations. It was decided that on an AWS
    VM the data from both locations would be collated. The awstats
    data will be transferred on an hourly basis to this VM. 

*** Where the service will run
    There were concerns on where the service should run. Initially it
    was being assumed that the service would run on the reverse proxy
    server itself. This was so because the stats file were located on
    this machine. By doing so it would reduce the overhead of
    transferring the stats files into another location and also the
    data that the service would use might be stale.  Setting up the
    service on the reverse proxy had its own security threats.  So,
    after a lot of discussion it was decided that the service will run
    on a separate VM on AWS.
*** Setup of the service
    As discussed in the [[Where%20the%20service%20will%20run][Where the service will run]], a VM on AWS will
    be created to run the service. The VM will be a part of the AWS
    cluster. The concern in setting up this VM is that the other VM's
    in the cluster are setup by ansible scripts. To setup this service
    ansible scripts need to be written. Manual entry of the FQDN and
    IP into some servers like reverse proxy and DNS may work
    temporarily but when the scripts are run, this information will be
    erased.  *Note*: Yet to be discussed.
** Procedure
   After a discussion (which also involved exchange of mails) the
   process of implementation was decided as follows:
   1. Pull all the data from deploy and AWS
      - Collate data from deploy and AWS
      - This data that will be used by the service using helper functions.
   2. Serving the data. 
      - Define end points based on the requirements
      - Build a service that implements these end points.
   3. Rendering of the data on vlab.co.in
      - Insert the JavaScript snippet in vlab.co.in
   4. Backup of all the statistics
      
   #+CAPTION:  Implementation diagram
   #+LABEL:  Implementation diagram
   [[./diagrams/file-flow-diagram.png]]

** Impediments

** Data collation
*** On AWS
-**** Generate ssh keypair on Ansible(DONE committed code to imp.)
-**** Copy the new id_rsa.pub generated with ansible access to analytics server(DONE comit to imp.)
-**** Move the generated key pair from ansible to reverse proxy under its .ssh dir(DONE comit to imp.)
-**** Changed the firewall rules on reverse proxy to allow outgoing ssh to analytics(DONE)
-**** Added cron jobs for hourly rsync preserving the relative path
- + @hourly rsync -arR /var/lib/awstats root@10.100.1.12:/root/aws/
- - skipped syncing the logs as Soumay reported she needed the flat
- stats file, another reason was chances of file corruption when the
- log is open.
-
-**** To rsync from base to analytics on the AWS cluster
- - Need to port forward TCP 2222 from router to analytics server on
- TCP 22(added to ansible imp.)
- - Also a outgoing rule from AWS router to port 22 of analytics
- - Copied a separate id_rsa.pub from http container from base into
- authorized_key of the analytics server, this is for passwordless
- rsync of stats file
-**** pending(**WIP** Adding to implementation, test remains)
- - Firewall rules on analytics server to allow ssh only from specific
- hosts(depends on the analytics server role)
-
-**** AWS router has additional iptables rules (**DONE** - added this to the imp.)
-#Forward incoming SSH connection on TCP port 2222 to Analytics server
--A PREROUTING -d 10.100.1.1 -p tcp -m tcp --dport 2222 -j DNAT --to-destination 10.100.1.12:22
-#Allow outgoing tcp on 22 to analytics server, port forwarding
--A OUTPUT -p tcp -m tcp id {{analytics_server_ip}} --dport 22 -j ACCEPT
-
-*** On BASE(**Manual** update the wiki)
-**** Setup cron on http container to rsync the stats file to stats.vlabs.ac.in on AWS cluster
- + @hourly rsync -azR -e "ssh -p 2222" /var/www/awstats/awstats* stats.vlabs.ac.in:/root/base/
-
-*** TODO
-**** Find a way to rsync after/before awstats update script, so the chances of file corruption are minimized
-
** Data extraction
   The code below is written to extract certain fields from the
   awstats text files.
#+BEGIN_SRC python :tangle extract_data.py
import sys
import re
import json
import os
import fnmatch

def get_file_names(year):
    list_of_files = []
    for file in os.listdir('.'):
        if fnmatch.fnmatch(file, '*'+ re.escape(year) + '*.txt'):
            list_of_files.append(file)
    return list_of_files

def extract_aws_data(year):
    files = get_file_names(year);
    dictionary = {}
    dict_tot_pages = {}
    for i in files:
        f = open(i)
        for line in f:
            if re.match(r'^BEGIN_DAY',line):
                n = int((line.strip('BEGIN_DAY ')).rstrip())
                x = 0
                p = 0
                for page_line in f:
                    if (x >= n):
                        break
                    p += int(page_line.split()[1])
                    x += 1
                f.close()
                dictionary[(i.lstrip('awstats.')).rstrip('.txt')] = p
                break
    z = 0
    for key in dictionary:
        z += dictionary[key]
#    print "total aws",z
    return z

def extract_iiit_data(year):
    files = get_file_names(year);
    dictionary = {}
    dict_tot_pages = {}
    for i in files:
        if re.search(r'awstats\d+.virtual-labs.ac.in.txt$',i):
            print i
            f = open(i)
            for line in f:
                if re.match(r'^BEGIN_DAY',line):
                    n = int((line.strip('BEGIN_DAY ')).rstrip())
                    x = 0
                    p = 0
                    for page_line in f:
                        if (x >= n):
                            break
                        p += int(page_line.split()[1])
                        x += 1
                    f.close()
                    dictionary[(i.lstrip('awstats.')).rstrip('.txt')] = p
                    break
    z = 0
    for key in dictionary:
        z += dictionary[key]
#    print "total iiit",z
    return z

def grand_total():
    year = ['2013', '2014', '2015' ]
    tot_pages=0
    for i in year:
        tot_pages += extract_iiit_data(i) + extract_aws_data(i)
    dict_tot_pages = {}
    dict_tot_pages["total-pages"]=tot_pages
#    print dict_tot_pages

grand_total()
#+END_SRC

** Display of data
 Following code dipicts the implementation of python-flask application as a service with endpoint =/numberofhits=. 
 this code will parse the data and extracts aws-stats and calculates the total numer of visits and return the value.
#+BEGIN_SRC python :tangle app.py
#flask imports
from flask import Flask, render_template, request, jsonify, make_response
import json
import os
#import config
from data import extract_data

# import the flask extension
from flask.ext.cache import Cache

app = Flask(__name__)

# define the cache config keys
app.config['CACHE_TYPE'] = 'simple'

# register the cache instance and binds it on to your app 
app.cache = Cache(app)

@app.route('/numberofhits')
@app.cache.cached(timeout=60)  # cache this view for 1 minute
def numberofhits():
    #curr_dir = os.path.dirname(os.path.realpath(__file__))
    #json_data = open(os.path.join(curr_dir, "data-2014.json"))
    #data = json.load(json_data)
    #json_string = json.dumps(data)
    numberofhits  = extract_data.grand_total()
    response = make_response(numberofhits)
    response.headers['Access-Control-Allow-Origin'] = '*'
    response.headers['Access-Control-Allow-Credentials'] = 'true'
    return response
    #return jsonify(data=json_string)

if __name__ == '__main__':
    app.run(port=5000, debug=True)
#+END_SRC


* Work Plan
** TODO Prepare the document
** DONE Discuss on how to meet the requirement
        There was a discussion on [2015-06-23 Tue] with the
        participants as Saurabh, Thirumal, Anon, Zubair, Soumya and
        the other VLEAD members. The task division and the process to
        bring up the service was also discussed which is mentioned in
        the [[Process%20for%20implementation][Process for implementation]] section.
** DONE Figure out the various location of the labs
        There are several locations the labs are situated namely AWS,
        deploy, separate containers. How to display the combined
        output of all the labs is yet to be decided.  (This has been
        done and the decision taken has been mentioned in the [[Present%20scenario%20of%20statistics][Present
        scenario of statistics]])

** TODO Build the scripts
** TODO Test the scripts.
** TODO Updation of vlab.co.in
   
