#+TITLE:     Building the Analytics service of Virtual Labs
#+AUTHOR:    M.S.Soumya
#+DATE:      2015-06-09 Tue
#+PROPERTY: results output
#+PROPERTY: exports code
#+SETUPFILE: org-templates/level-0.org

* Introduction
  
  The analytics of Virtual-labs are currently generated by
  awstats. Awstats provides a dashboard for viewing these statistics.
  It provides a layman with too much information to comprehend.  A
  simple dashboard which displays the total views and hits of the site
  is required. This would help keep us informed on the usage of our
  labs.

* Requirements
** Current requirement
   The pages views count from both AWS and local deployment must be
   displayed on vlab.co.in
** Extended requirement
   Possiblities 

  
* COMMENT Objective
  The objective of this exercise is to display the usage of
  virtual-labs in terms of number of pages viewed on vlab.co.in.

* Design
  Awstats generates parsed text files which contain the statistics in
  text format. The required data from these text files needs to be
  extracted and displayed on a HTML page.To achieve this we build the
  service.  
  
  *Note*: Currently the HTML page is on vlab.co.in home page. In
  future a button / hyperlink could also be incorporated which would
  display a page that shows slightly more details.
   
  #+CAPTION:  Design diagram
  #+LABEL:  Design diagram
  [[./diagrams/analytics.png]]
 
  To build a web service which would display certain specific
  information from the statistics generated by awstats. As the diagram
  suggests, this service would have a URL which would be hyper-linked
  to a button on the vlab.co.in page.  On clicking the button this
  service would get invoked and the information is displayed on the
  page. This would be a dynamic process and any changes in the
  statistics must be incorporated by the service.

* Implementation
** Present status of statistics
   Currently the labs are hosted in 3 different locations namely:
   + AWS ( Currently 63 labs)
   + Containers at VLEAD (Around 20 labs) 
   + Rest of the labs directed by vlab.co.in (either hosted by Amrita
     or individual institutes)
   
   The statistics of the labs hosted on AWS and the containers at
   VLEAD can be obtained because Awstats was configured and the
   statistics file are accessible. The statistics of the rest of the
   labs which are directed by vlab.co.in (not on AWS or on the VLEAD
   containers) cannot be obtained.

** Decisions
*** Collation of statistics
    The statistics are present in two different locations AWS and on
    the local reverseproxy at VLEAD. To get numbers the data is
    required from both these locations. It was decided that on an AWS
    VM the data from both locations would be collated. The awstats
    data will be transferred on an hourly basis to this VM. 

*** Where the service will run
    There were concerns on where the service should run. Initially it
    was being assumed that the service would run on the reverse proxy
    server itself. This was so because the stats file were located on
    this machine. By doing so it would reduce the overhead of
    transferring the stats files into another location and also the
    data that the service would use might be stale.  Setting up the
    service on the reverse proxy had its own security threats.  So,
    after a lot of discussion it was decided that the service will run
    on a separate VM on AWS.
*** Setup of the service
    As discussed in the [[Where%20the%20service%20will%20run][Where the service will run]], a VM on AWS will
    be created to run the service. The VM will be a part of the AWS
    cluster. The concern in setting up this VM is that the other VM's
    in the cluster are setup by ansible scripts. To setup this service
    ansible scripts need to be written. Manual entry of the FQDN and
    IP into some servers like reverse proxy and DNS may work
    temporarily but when the scripts are run, this information will be
    erased.  *Note*: Yet to be discussed.
** Procedure
   After a discussion (which also involved exchange of mails) the
   process of implementation was decided as follows:
   1. Pull all the data from deploy and AWS
      - Collate data from deploy and AWS
      - This data that will be used by the service using helper functions.
   2. Serving the data. 
      - Define end points based on the requirements
      - Build a service that implements these end points.
   3. Rendering of the data on vlab.co.in
      - Insert the JavaScript snippet in vlab.co.in
   4. Backup of all the statistics
      
   #+CAPTION:  Implementation diagram
   #+LABEL:  Implementation diagram
   [[./diagrams/file-flow-diagram.png]]

** Impediments
   
** Data collation

   We have two different sources, a reverse proxy on AWS
cluster(Source 1) and a reverse proxy on the base machines in IIIT-H
(Source 2). These sources are geographically located in different
continents and are on different networks.

   We need a way of transferring the data(statistics) to the analytics
server where it will be processed. We have used =rsync= to transfer,
and have setup periodic transfer jobs using =cronie= which executes
every hour.

As this task involves collecting data from different sources, like a
cluster which is managed with configuration management tool =Ansible=
and another one a manually configured server, data collation is also a
mix of automation and manual steps.

We now present the configuration procedure of both.

*** Configuring AWS cluster
    Here we need the following:
 - Analytics server configured to accept =rsync= over =TCP=
 - Reverse Proxy(Source 1) configured to push the generated statistics
   to =Analytics= server at regular intervals
 - Router cofigured to allow incoming =rsync= connections from Source
   2 to Analytics server
 
 As AWS cluster is configured by =Ansible=, we have scripts to achieve
 the above via =Ansible=.

*** On BASE
 Here we manually setup:
 - Reverse Proxy(Source 2) configued to push generated statistics to
   =Analytics= server every hour

*** TODO
 -  Find a way to rsync after/before awstats update script, so the
    chances of file corruption are minimized.

** Data extraction
   The code below is written to extract certain fields from the
   awstats text files.
#+BEGIN_SRC python :tangle extract_data.py
import sys
import re
import json
import os
import fnmatch

def get_file_names(year):
    list_of_files = []
    for file in os.listdir('.'):
        if fnmatch.fnmatch(file, '*'+ re.escape(year) + '*.txt'):
            list_of_files.append(file)
    return list_of_files

def extract_aws_data(year):
    files = get_file_names(year);
    dictionary = {}
    dict_tot_pages = {}
    for i in files:
        f = open(i)
        for line in f:
            if re.match(r'^BEGIN_DAY',line):
                n = int((line.strip('BEGIN_DAY ')).rstrip())
                x = 0
                p = 0
                for page_line in f:
                    if (x >= n):
                        break
                    p += int(page_line.split()[1])
                    x += 1
                f.close()
                dictionary[(i.lstrip('awstats.')).rstrip('.txt')] = p
                break
    z = 0
    for key in dictionary:
        z += dictionary[key]
#    print "total aws",z
    return z

def extract_iiit_data(year):
    files = get_file_names(year);
    dictionary = {}
    dict_tot_pages = {}
    for i in files:
        if re.search(r'awstats\d+.virtual-labs.ac.in.txt$',i):
            print i
            f = open(i)
            for line in f:
                if re.match(r'^BEGIN_DAY',line):
                    n = int((line.strip('BEGIN_DAY ')).rstrip())
                    x = 0
                    p = 0
                    for page_line in f:
                        if (x >= n):
                            break
                        p += int(page_line.split()[1])
                        x += 1
                    f.close()
                    dictionary[(i.lstrip('awstats.')).rstrip('.txt')] = p
                    break
    z = 0
    for key in dictionary:
        z += dictionary[key]
#    print "total iiit",z
    return z

def grand_total():
    year = ['2013', '2014', '2015' ]
    tot_pages=0
    for i in year:
        tot_pages += extract_iiit_data(i) + extract_aws_data(i)
    dict_tot_pages = {}
    dict_tot_pages["total-pages"]=tot_pages
#    print dict_tot_pages

grand_total()
#+END_SRC

** Display of data
 Following code shows the implementation of python-flask application as a service with endpoint =/numberofhits=. 
 This code will parse the data, extracts aws-stats, calculates and returns the total number of visits.
#+BEGIN_SRC python :tangle app.py
from flask import Flask, make_response
#+END_SRC
First import the Flask class. An instance of this class will be our Web Server Gateway Interface(WSGI) application.
#+BEGIN_SRC 
import json
import os
#import config
from data import extract_data
# import the flask extension
from flask.ext.cache import Cache
#+END_SRC
#+BEGIN_SRC 
app = Flask(__name__)
#+END_SRC
Next, create an instance of the class(Flask). The first argument is the name of the application’s module or package. 
If single module is used then, use *__name__* because it depends on if it’s started as 
application or imported as module the name will be different. 
This is needed so that Flask knows where to look for templates, static files, and so on. 
#+BEGIN_SRC
app.config['CACHE_TYPE'] = 'simple'
#+END_SRC
 define the cache config keys
#+BEGIN_SRC 
app.cache = Cache(app)
#+END_SRC
register the cache instance and binds it on to your app 
#+BEGIN_SRC 
@app.route('/numberofhits')
#+END_SRC
Then use the =route()= decorator to tell Flask what URL should trigger the function.
#+BEGIN_SRC 
@app.cache.cached(timeout=60)  
#+END_SRC
cache this view for 1 minute
#+BEGIN_SRC 
def numberofhits():
#+END_SRC
The function is given a name which is also used to generate URLs for that particular function, 
and returns the value we want to display in the browser.
#+BEGIN_SRC 
    numberofhits  = extract_data.grand_total()
#+END_SRC
call the function =grand_total()= which is defined in =extract_data.py= to get the total number of hits.
#+BEGIN_SRC
    response = make_response(numberofhits)
    response.headers['Access-Control-Allow-Origin'] = '*'
    response.headers['Access-Control-Allow-Credentials'] = 'true'
    return response
#+END_SRC
=make_response()= function can be called instead of using a return 
and will get a response object which can be used to attach headers.
=Access-Control-Allow-Origin= is a CORS (Cross-Origin Resource Sharing) header.
When Site A tries to fetch content from Site B, Site B can send an =Access-Control-Allow-Origin= 
response header to tell the browser that the content of this page is accessible to certain origins.
By default, Site B's pages are not accessible to any other origin, using the =Access-Control-Allow-Origin= 
header opens a door for cross-origin access by specific requesting origins.
The server can give permission to include cookies by setting the =Access-Control-Allow-Credentials= header.
#+BEGIN_SRC 
if __name__ == '__main__':
    app.run(port=5000, debug=True)
#+END_SRC
Finally we use the =run()= function to run the local server with our application. The if *__name__ == '__main__':* 
makes sure the server only runs if the script is executed directly from the Python interpreter and not 
used as an imported module.
*Note:* To stop the server, hit control-C

* Test Cases
A test case is a document, which has a set of test data, preconditions, expected results and postconditions, 
developed for a particular test scenario in order to verify compliance against a specific requirement.  
** Test case ID: TC01
*** Objective
    The objective of this test case is to test whether the application is running or not by sending HTTP GET request and 
    also to test the return value of the response.
*** Apparatus
    - A machine installed with =Flask-Testing=.
*** Theory
    This test case will test the funtionality of endpoint =/numberofhits= where it will return the total number of pages visited.
*** Procedure.
    - Run =python test_app.py= in terminal.
    - Check the output of this command =Ran 1 test in 0.034s= and =OK=.
    - If the above output is found then test is passed.
*** Experiment
#+BEGIN_SRC python :tangle test_app.py
import unittest
from flask.ext.testing import TestCase
import config
from app import app
#+END_SRC

#+BEGIN_SRC 
class AnalyticsTestCase(unittest.TestCase):
#+END_SRC
A testcase is created by subclassing =unittest.TestCase=. 
#+BEGIN_SRC 
    def test_numberofhits(self):
        json_string = config.grand_total()
        print json_string
#+END_SRC
The tests are defined with methods whose names start with the letters =test=. 
This naming convention informs the test runner about which methods represent tests.
To start testing the functionality of the application, add a new test method to our class, 
like =test_numberofhits()=.Similarly, other test methods can be added.
#+BEGIN_SRC 
        tester = app.test_client(self)
        response = tester.get('http://localhost:5000/numberofhits')
        self.assertEqual(response.status_code, 200)
        self.assertEqual(json_string,  response.data)

#+END_SRC            
Each test is a call to =assertEqual()= function which will test that first and second arguments are equal. 
If the values do not compare equal, the test will fail.
#+BEGIN_SRC 
if __name__ == '__main__':
    unittest.main()
#+END_SRC
The final block shows a simple way to run the tests. =unittest.main()= 
provides a command-line interface to the test script. 
When run from the command line, the above script produces an output.   

#+BEGIN_SRC python:tangle config.py
#!/usr/bin/python
def grand_total():
  dict = {'numberofhits':'1234'};
  return dict['numberofhits']
#+END_SRC
The above code will return the userdefined value when =grand_toal()= function is called.
*** Result
 #+BEGIN_SRC 
$ python test_app.py
.
----------------------------------------------------------------------
Ran 1 test in 0.034s

OK
#+END_SRC
* Work Plan
** TODO Prepare the document
** DONE Discuss on how to meet the requirement
        There was a discussion on [2015-06-23 Tue] with the
        participants as Saurabh, Thirumal, Anon, Zubair, Soumya and
        the other VLEAD members. The task division and the process to
        bring up the service was also discussed which is mentioned in
        the [[Process%20for%20implementation][Process for implementation]] section.
** DONE Figure out the various location of the labs
        There are several locations the labs are situated namely AWS,
        deploy, separate containers. How to display the combined
        output of all the labs is yet to be decided.  (This has been
        done and the decision taken has been mentioned in the [[Present%20scenario%20of%20statistics][Present
        scenario of statistics]])

** DONE Build the scripts
** DONE Test the scripts.
** TODO Updation of vlab.co.in
   
